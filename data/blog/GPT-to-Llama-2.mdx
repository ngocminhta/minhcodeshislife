---
title: 'Tá»« GPT-3 Ä‘áº¿n LLaMA-2: CÃº tÃ¡t cá»§a Mark xoÄƒn vÃ o OpenAI'
date: '2024-01-15'
lastmod: '2023-01-15'
tags: ['deep learning', 'transformer', 'gpt', 'llama']
draft: false
summary: 'LLaMA-2 Ä‘Æ°á»£c tung ra vá»›i má»™t paper dÃ i 77 trang mÃ´ táº£ Ä‘áº§y Ä‘á»§ vá» táº¥t cáº£ kÄ© thuáº­t sá»­ dá»¥ng vÃ  má»Ÿ cho cáº£ má»¥c Ä‘Ã­ch thÆ°Æ¡ng máº¡i.'
images: ['/static/images/GPT-to-LLaMA-2/llama.jepg']
---

> ğŸ’¡ Trong blog nÃ y, mÃ¬nh sáº½ trÃ¬nh bÃ y tÃ³m gá»n 77 trang paper cá»§a Meta vá» model nÃ y. LLaMA-2 lÃ  phiÃªn báº£n thá»© 2 cá»§a LLaMa - má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n Ä‘Æ°á»£c táº¡o ra bá»Ÿi Facebook AI Research. MÃ´ hÃ¬nh cÃ³ kiáº¿n trÃºc tÆ°Æ¡ng tá»± nhÆ° LLaMa nhÆ°ng Ä‘Æ°á»£c bá»• sung thÃªm dá»¯ liá»‡u, cáº£i thiá»‡n cháº¥t lÆ°á»£ng vÃ  cÃ³ thÃªm cÃ¡c phÆ°Æ¡ng phÃ¡p tá»‘i Æ°u má»›i Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t cao hÆ¡n. BÃ i viáº¿t nÃ y Ä‘Æ°á»£c mÃ¬nh viáº¿t dÆ°á»›i sá»± tham kháº£o blog cá»§a anh Pháº¡m VÄƒn ToÃ n, AI Engineer Leader táº¡i Sun* Vietnam.

# Kiáº¿n trÃºc mÃ´ hÃ¬nh

Trong bÃ i bÃ¡o, nhÃ³m tÃ¡c giáº£ khÃ´ng cung cáº¥p thÃ´ng tin chi tiáº¿t vá» cáº¥u trÃºc mÃ´ hÃ¬nh, chá»‰ tiáº¿t lá»™ ráº±ng mÃ´ hÃ¬nh tuÃ¢n theo cáº¥u trÃºc Transformer chuáº©n vÃ  tÆ°Æ¡ng tá»± nhÆ° LLaMA-1. Äá»ƒ hiá»ƒu rÃµ hÆ¡n vá» cáº¥u trÃºc cá»§a LLaMA-2, chÃºng ta cáº§n xem xÃ©t cáº¥u trÃºc cá»§a mÃ´ hÃ¬nh LLaMa.

Cáº£ LLaMa vÃ  LLaMA-2 Ä‘á»u lÃ  cÃ¡c Generative Pretrained Transformer dá»±a trÃªn kiáº¿n trÃºc Transformer. Source code cÃ³ thá»ƒ Ä‘Æ°á»£c tham kháº£o táº¡i Ä‘Ã¢y. Tuy nhiÃªn, cÃ³ má»™t sá»‘ Ä‘iá»ƒm khÃ¡c biá»‡t cÆ¡ báº£n so vá»›i cáº¥u trÃºc GPT tiÃªu chuáº©n:

* LLaMa sá»­ dá»¥ng RMSNorm Ä‘á»ƒ chuáº©n hoÃ¡ Ä‘áº§u vÃ o cho má»—i lá»›p transformer thay vÃ¬ Ä‘áº§u ra.
* Sá»­ dá»¥ng SwiGLU activation thay vÃ¬ ReLu Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t quÃ¡ trÃ¬nh huáº¥n luyá»‡n.
* Sá»­ dá»¥ng rotary positional embeddings (RoPE) trong cÃ¡c lá»›p máº¡ng tÆ°Æ¡ng tá»± nhÆ° trong GPT-Neo-X.

Theo bÃ i bÃ¡o cá»§a LLaMA-2, sá»± thay Ä‘á»•i duy nháº¥t trong cáº¥u trÃºc náº±m á»Ÿ kÃ­ch thÆ°á»›c cá»§a context length vÃ  sá»­ dá»¥ng grouped-query attention. Viá»‡c tÄƒng kÃ­ch thÆ°á»›c cá»§a context length giÃºp cho mÃ´ hÃ¬nh cÃ³ thá»ƒ xá»­ lÃ½ nhiá»u thÃ´ng tin hÆ¡n, Ä‘áº·c biá»‡t thuáº­n lá»£i cho viá»‡c xá»­ lÃ½ cÃ¡c vÄƒn báº£n dÃ i. Thay vÃ¬ sá»­ dá»¥ng multi-head attention nhÆ° trong Transformer tiÃªu chuáº©n, viá»‡c chuyá»ƒn sang grouped-query attention vá»›i 8 key-value projection giÃºp tÄƒng tá»‘c Ä‘á»™ huáº¥n luyá»‡n vÃ  dá»… dÃ ng tÄƒng Ä‘á»™ phá»©c táº¡p cá»§a mÃ´ hÃ¬nh cÅ©ng nhÆ° tÄƒng batch size vÃ  context length.

TrÆ°á»›c khi Ä‘i vÃ o viá»‡c huáº¥n luyá»‡n mÃ´ hÃ¬nh, chÃºng ta cÃ¹ng tÃ¬m hiá»ƒu qua cÃ¡ch thá»©c mÃ  cÃ¡c kÄ© sÆ° Meta xá»­ lÃ­ vÃ  thu tháº­p dá»¯ liá»‡u há» cÃ³ Ä‘Æ°á»£c nhÃ©.

# CÃ¡ch xá»­ lÃ­ dá»¯ liá»‡u

Má»™t Ä‘iá»ƒm Ä‘Ã¡ng chÃº Ã½ cá»§a paper nÃ y Ä‘Ã³ chÃ­nh lÃ  Meta Ä‘Ã£ cÃ´ng khai thá»«a nháº­n má»™t yáº¿u tá»‘ quan trá»ng nháº¥t trong viá»‡c huáº¥n luyá»‡n LLM vá»›i phÆ°Æ¡ng phÃ¡p RLHF Ä‘Ã³ chÃ­nh lÃ  Reward Modeling. ChÃºng ta cÅ©ng Ä‘Ã£ biáº¿t ráº±ng, thuáº­t toÃ¡n Reinforcement learning sáº½ cáº§n má»™t reward function. CÃ³ nhiá»u bÃ i toÃ¡n ráº¥t dá»… Ä‘á»ƒ Ä‘Æ°a ra reward nhÆ°ng cÅ©ng cÃ³ nhá»¯ng bÃ i toÃ¡n ráº¥t khÃ³ Ä‘á»ƒ Ä‘Æ°a ra nhÆ°ng viá»‡c Ä‘Ã¡nh giÃ¡ text sinh ra cÃ³ há»¯u Ã­ch hay khÃ´ng cháº³ng háº¡n. NÃ³ lÃ  má»™t yáº¿u tá»‘ háº¿t sá»©c Ä‘á»‹nh tÃ­nh. ChÃ­nh vÃ¬ tháº¿ Ä‘á»ƒ thiáº¿t káº¿ ra Ä‘Æ°á»£c má»™t reward funciton chuáº©n cho bÃ i toÃ¡n Ä‘Ã¡nh giÃ¡ text nÃ y, Meta Ä‘Ã£ ráº¥t hao tÃ¢m tá»‘n sá»©c Ä‘á»ƒ táº¡o ra Ä‘Æ°á»£c cÃ¡c táº­p dá»¯ liá»‡u mÃ  cÃ³ reward cao theo cÃ¡ch hiá»ƒu cá»§a con ngÆ°á»i. Táº­p dá»¯ liá»‡u nÃ y gá»i lÃ  preference data.

á» Ä‘Ã¢y mÃ¬nh xin tÃ³m táº¯t láº¡i má»™t vÃ i Ä‘iá»ƒm chÃ­nh trong cÃ¡ch lÃ m dá»¯ liá»‡u cá»§a há» nhÆ° sau:

* Thu tháº­p cÃ¡c so sÃ¡nh nhá»‹ phÃ¢n tá»« ngÆ°á»i gÃ¡n nhÃ£n, cÃ³ nghÄ©a lÃ  cho má»—i Ä‘á» má»¥c nháº­p, há» sáº½ cung cáº¥p hai cÃ¢u tráº£ lá»i tÆ°Æ¡ng á»©ng. NgÆ°á»i gÃ¡n nhÃ£n sáº½ Ä‘Ã¡nh giÃ¡ hai cÃ¢u tráº£ lá»i nÃ y vÃ  lá»±a chá»n má»™t trong hai. Nhá»¯ng ngÆ°á»i gÃ¡n nhÃ£n cÅ©ng sáº½ Ä‘Æ°a ra cÃ¡c má»©c Ä‘Ã¡nh giÃ¡ Ä‘á»‹nh tÃ­nh nhÆ° "significantly better", "better", "slightly better", hoáº·c "negligibly better/unsure".

* Viá»‡c sá»­ dá»¥ng Æ°u tiÃªn Ä‘a lÆ°á»£t (multi-turn preferences) Ä‘á»“ng nghÄ©a vá»›i viá»‡c lá»±a chá»n cÃ¡c cÃ¢u tráº£ lá»i tá»« cÃ¡c checkpoints khÃ¡c nhau cá»§a mÃ´ hÃ¬nh, káº¿t há»£p vá»›i viá»‡c Ä‘iá»u chá»‰nh tham sá»‘ temperature Ä‘á»ƒ táº¡o ra sá»± Ä‘a dáº¡ng trong cÃ¡c cÃ¢u tráº£ lá»i cho má»™t Ä‘á» má»¥c. Viá»‡c tÄƒng cÆ°á»ng tÃ­nh Ä‘a dáº¡ng nÃ y ráº¥t cÃ³ Ã­ch trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n mÃ´ hÃ¬nh vá»›i RLHF trong tÆ°Æ¡ng lai.

* Táº­p trung vÃ o viá»‡c giáº£i quyáº¿t hai váº¥n Ä‘á» mÃ  há» hy vá»ng LLaMA-2 sáº½ Ä‘em láº¡i, Ä‘Ã³ lÃ  sá»± há»¯u Ã­ch vÃ  an toÃ n, vÃ  Ã¡p dá»¥ng hai nguyÃªn táº¯c hÆ°á»›ng dáº«n riÃªng biá»‡t cho má»—i nhÃ  cung cáº¥p dá»¯ liá»‡u. Há» Æ°u tiÃªn tÃ­nh an toÃ n cá»§a cÃ¡c cÃ¢u tráº£ lá»i Ä‘Æ°á»£c táº¡o ra bá»Ÿi mÃ´ hÃ¬nh hÆ¡n háº¿t. Trong bÃ i bÃ¡o, cÃ¡c tÃ¡c giáº£ sá»­ dá»¥ng thÃ´ng tin an toÃ n (safety metadata) trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n vÃ  Ä‘áº£m báº£o ráº±ng khÃ´ng cÃ³ dá»¯ liá»‡u khÃ´ng an toÃ n nÃ o Ä‘Æ°á»£c sá»­ dá»¥ng trong quÃ¡ trÃ¬nh Ä‘iá»u chá»‰nh (fine-tuning). Há» khÃ´ng Ä‘á» cáº­p cá»¥ thá»ƒ vá» cÃ¡ch cÃ¡c thÃ´ng tin an toÃ n nÃ y Ä‘Æ°á»£c táº¡o ra cho má»¥c Ä‘Ã­ch nÃ o khÃ¡c ngoÃ i viá»‡c Ä‘áº£m báº£o an toÃ n, cÅ©ng nhÆ° khÃ´ng Ä‘á» cáº­p Ä‘áº¿n cÃ¡c loáº¡i thÃ´ng tin metadata khÃ¡c trong dá»¯ liá»‡u. Tuy nhiÃªn, cÃ³ láº½ cÃ³ má»™t sá»‘ loáº¡i thÃ´ng tin metadata khÃ¡c nhÆ° cÃ¡c Ä‘á» má»¥c dá»… gÃ¢y nháº§m láº«n.

Há» tiáº¿n hÃ nh viá»‡c thu tháº­p dá»¯ liá»‡u má»™t cÃ¡ch liÃªn tá»¥c theo tá»«ng lÃ´ hÃ ng hÃ ng tuáº§n Ä‘á»ƒ Ä‘áº£m báº£o quáº£n lÃ½ vÃ  phÃ¢n phá»‘i dá»¯ liá»‡u hiá»‡u quáº£. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  sau má»—i tuáº§n, má»™t lÃ´ dá»¯ liá»‡u má»›i sáº½ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ huáº¥n luyá»‡n reward model vÃ  chat model, nháº±m Ä‘áº£m báº£o ráº±ng khÃ´ng cÃ³ sá»± chÃªnh lá»‡ch Ä‘Ã¡ng ká»ƒ vá» phÃ¢n phá»‘i dá»¯ liá»‡u. Khi reward model Ä‘Æ°á»£c cáº£i thiá»‡n, chat model cÅ©ng sáº½ Ä‘Æ°á»£c nÃ¢ng cáº¥p tÆ°Æ¡ng á»©ng Ä‘á»ƒ Ä‘áº£m báº£o cháº¥t lÆ°á»£ng.

# CÃ¡ch huáº¥n luyá»‡n mÃ´ hÃ¬nh

## CÃ¡c kÄ© thuáº­t Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ huáº¥n luyá»‡n

ÄÃ¢y cÃ³ thá»ƒ Ä‘Æ°á»£c xem nhÆ° lÃ  báº£n cháº¥t cá»§a viá»‡c huáº¥n luyá»‡n mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n. NhÆ° Ä‘Ã£ Ä‘á» cáº­p á»Ÿ trÃªn, Ä‘iá»ƒm quan trá»ng nháº¥t khÃ´ng pháº£i lÃ  cáº¥u trÃºc cá»§a mÃ´ hÃ¬nh mÃ  lÃ  dá»¯ liá»‡u vÃ  phÆ°Æ¡ng phÃ¡p huáº¥n luyá»‡n mÃ´ hÃ¬nh. Vá» dá»¯ liá»‡u, chÃºng ta sáº½ tháº£o luáº­n sau, nhÆ°ng trÆ°á»›c tiÃªn, mÃ¬nh muá»‘n Ä‘á» cáº­p Ä‘áº¿n ká»¹ thuáº­t huáº¥n luyá»‡n vÃ  Ä‘iá»u chá»‰nh mÃ´ hÃ¬nh ngÃ´n ngá»¯ (LLM) báº±ng ká»¹ thuáº­t RLHF. ÄÃ¢y lÃ  yáº¿u tá»‘ quan trá»ng trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n cá»§a LLaMA-2, má»™t pháº§n mÃ  mÃ¬nh Ä‘Ã£ nghe nhiá»u nhÆ°ng chÆ°a tháº¥y má»™t bÃ i bÃ¡o nÃ o giáº£i thÃ­ch cá»¥ thá»ƒ cÃ¡ch triá»ƒn khai cho Ä‘áº¿n khi LLaMA-2 xuáº¥t hiá»‡n. Tá»•ng quan vá» quÃ¡ trÃ¬nh huáº¥n luyá»‡n vÃ  Ä‘iá»u chá»‰nh mÃ´ hÃ¬nh báº±ng RLHF cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¬m tháº¥y trong hÃ¬nh sau.

ChÃºng ta cÃ³ 3 bÆ°á»›c á»Ÿ trong viá»‡c training LLaMA-2, bao gá»“m: pre-training, supervised fine-tuning vÃ  cuá»‘i cÃ¹ng lÃ  RLHF (**R**einforcement **L**earning with **H**uman **F**eedbacks).

### Pre-training

Äáº§u tiÃªn lÃ  giai Ä‘oáº¡n Pretraining - quÃ¡ trÃ¬nh huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh transformer trÃªn má»™t táº­p dá»¯ liá»‡u cá»±c ká»³ lá»›n báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p há»c khÃ´ng giÃ¡m sÃ¡t tá»± chá»§. Ká»¹ thuáº­t nÃ y, Ä‘Æ°á»£c sá»­ dá»¥ng phá»• biáº¿n, giÃºp mÃ´ hÃ¬nh há»c tá»« dá»¯ liá»‡u báº±ng cÃ¡ch thá»±c hiá»‡n cÃ¡c biáº¿n Ä‘á»•i trá»±c tiáº¿p trÃªn dá»¯ liá»‡u. VÃ­ dá»¥, cÃ³ thá»ƒ che Ä‘i má»™t sá»‘ tá»« trong cÃ¢u Ä‘á»ƒ mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n tá»« bá»‹ che. Äiá»ƒm quan trá»ng á»Ÿ Ä‘Ã¢y lÃ  viá»‡c sá»­ dá»¥ng khoáº£ng 40% dá»¯ liá»‡u káº¿t há»£p vá»›i cÃ¡c nguá»“n dá»¯ liá»‡u cÃ³ sáºµn vÃ  há» ráº¥t cá»‘ gáº¯ng Ä‘á»ƒ loáº¡i bá» thÃ´ng tin cÃ¡ nhÃ¢n cÃ³ thá»ƒ gÃ¢y háº¡i cho mÃ´ hÃ¬nh. Do Ä‘Ã³, táº­p dá»¯ liá»‡u nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c coi lÃ  khÃ¡ sáº¡ch. Táº­p dá»¯ liá»‡u nÃ y chá»©a khoáº£ng 2 nghÃ¬n tá»‰ token. Há» huáº¥n luyá»‡n mÃ´ hÃ¬nh Transformer báº±ng thuáº­t toÃ¡n AdamW vÃ  sá»­ dá»¥ng lá»‹ch trÃ¬nh tá»‘c Ä‘á»™ há»c vá»›i 2000 bÆ°á»›c "warmup" ban Ä‘áº§u.

Äiá»u thÃº vá»‹ lÃ  sau khi huáº¥n luyá»‡n vá»›i 2000 tá»‰ token, training loss váº«n chÆ°a cho tháº¥y dáº¥u hiá»‡u bÃ£o hÃ²a, tá»©c lÃ  viá»‡c bá»• sung thÃªm dá»¯ liá»‡u vÃ  thá»i gian huáº¥n luyá»‡n cÃ³ thá»ƒ táº¡o ra má»™t mÃ´ hÃ¬nh tá»‘t hÆ¡n. Tuy nhiÃªn, phiÃªn báº£n hiá»‡n táº¡i cá»§a LLaMa Ä‘Ã£ vÆ°á»£t trá»™i so vá»›i táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh mÃ£ nguá»“n má»Ÿ khÃ¡c trÃªn cÃ¡c chá»‰ sá»‘ benchmark khÃ¡c nhau. NÃ³ cÅ©ng cÃ³ káº¿t quáº£ cáº¡nh tranh vá»›i cÃ¡c mÃ´ hÃ¬nh LLM Ä‘Ã³ng (closed source) nhÆ° ChatGPT, nhÆ°ng váº«n cÃ¡ch xa so vá»›i GPT-4. Äiá»u nÃ y cÅ©ng dá»… hiá»ƒu bá»Ÿi khÃ´ng cÃ³ sáº£n pháº©m nÃ o cÃ³ thá»ƒ Ä‘áº£m báº£o cháº¥t lÆ°á»£ng, chi phÃ­ tháº¥p (tháº­m chÃ­ lÃ  miá»…n phÃ­) vÃ  Ä‘Æ°á»£c nhiá»u ngÆ°á»i sá»­ dá»¥ng.

### Supervised Fine-tuning

TÃ­nh cháº¥t lÆ°á»£ng cá»§a dá»¯ liá»‡u Ä‘Ã³ng vai trÃ² quan trá»ng nháº¥t trong viá»‡c fine-tuning. Tuy nhiÃªn, viá»‡c sá»­ dá»¥ng hÃ ng triá»‡u dá»¯ liá»‡u SFT tá»« nhiá»u nguá»“n khÃ¡c nhau cá»§a bÃªn thá»© ba cÃ³ thá»ƒ gáº·p pháº£i váº¥n Ä‘á» vá» Ä‘a dáº¡ng vÃ  cháº¥t lÆ°á»£ng. Thay vÃ o Ä‘Ã³, tÃ¡c giáº£ táº­p trung vÃ o viá»‡c sá»­ dá»¥ng má»™t táº­p nhá» hÆ¡n nhÆ°ng cháº¥t lÆ°á»£ng dá»¯ liá»‡u cao hÆ¡n tá»« Ä‘á»™i ngÅ© lÃ m dá»¯ liá»‡u cá»§a mÃ¬nh. Káº¿t quáº£ cho tháº¥y chá»‰ cáº§n khoáº£ng 10000 máº«u dá»¯ liá»‡u Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u quáº£ tá»‘t. Sau khi cÃ³ mÃ´ hÃ¬nh pre-trained, tÃ¡c giáº£ tiáº¿n hÃ nh Ä‘iá»u chá»‰nh sang dá»¯ liá»‡u dáº¡ng chat. Há» sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p instruction tuning vá»›i cÃ¡c dá»¯ liá»‡u cÃ´ng cá»™ng Ä‘á»ƒ lÃ m má»“i cho mÃ´ hÃ¬nh. QuÃ¡ trÃ¬nh Ä‘iá»u chá»‰nh Ä‘Æ°á»£c thá»±c hiá»‡n vá»›i batch size lÃ  64 vÃ  Ä‘á»™ dÃ i chuá»—i lÃ  4096. Äá»ƒ Ä‘áº£m báº£o ráº±ng Ä‘á»™ dÃ i chuá»—i cá»§a mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘iá»n Ä‘áº§y Ä‘á»§, táº¥t cáº£ cÃ¡c cÃ¢u há»i vÃ  cÃ¢u tráº£ lá»i tá»« táº­p huáº¥n luyá»‡n Ä‘Æ°á»£c ghÃ©p láº¡i vá»›i nhau báº±ng má»™t token Ä‘áº·c biá»‡t Ä‘á»ƒ phÃ¢n tÃ¡ch pháº§n cÃ¢u há»i vÃ  pháº§n cÃ¢u tráº£ lá»i. QuÃ¡ trÃ¬nh back propagation chá»‰ Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn cÃ¡c token trong pháº§n cÃ¢u tráº£ lá»i. MÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘iá»u chá»‰nh trong 2 epochs.

### RLHF

TrÆ°á»›c háº¿t mÃ¬nh muá»‘n nÃ³i Ä‘áº¿n má»¥c tiÃªu cá»§a huáº¥n luyá»‡n LLM lÃ  Ä‘á»ƒ cÃ³ thá»ƒ alignment hay cÃ²n gá»i lÃ  Ä‘iá»u chá»‰nh mÃ´ hÃ¬nh cho phÃ¹ há»£p vá»›i sá»Ÿ thÃ­ch vÃ  hÃ nh vi cá»§a con ngÆ°á»i. Äá»ƒ thá»±c hiá»‡n cÄƒn chá»‰nh nÃ y ngÆ°á»i ta Ä‘á» xuáº¥t ra kÄ© thuáº­t RLHF. Trong RLHF chÃºng ta cáº§n quan tÃ¢m Ä‘áº¿n hai yáº¿u tá»‘:

* **Reinforcement learning**: ÄÃ¢y lÃ  má»™t ká»¹ thuáº­t trong lÄ©nh vá»±c há»c mÃ¡y Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh nháº±m tá»‘i Ä‘a hÃ³a má»™t pháº§n thÆ°á»Ÿng (reward) cá»¥ thá»ƒ. Trong ngá»¯ cáº£nh cá»§a LLM, "pháº§n thÆ°á»Ÿng" á»Ÿ Ä‘Ã¢y cÃ³ thá»ƒ Ä‘Æ°á»£c hiá»ƒu lÃ  pháº£n á»©ng cáº£m xÃºc cá»§a con ngÆ°á»i, biá»ƒu hiá»‡n qua sá»± thÃ­ch thÃº, sá»± kinh ngáº¡c, sá»± há»¯u Ã­ch hoáº·c sá»± tá»± nhiÃªn mÃ  há» cáº£m nháº­n khi giao tiáº¿p vá»›i chatbot. Tuy nhiÃªn, má»™t thÃ¡ch thá»©c lá»›n trong viá»‡c Ã¡p dá»¥ng RL vÃ o bÃ i toÃ¡n LLM lÃ  viá»‡c xÃ¡c Ä‘á»‹nh hÃ m pháº§n thÆ°á»Ÿng (reward function). Trong khi má»™t sá»‘ trÆ°á»ng há»£p nhÆ° chÆ¡i trÃ² chÆ¡i cÃ³ thá»ƒ dá»… dÃ ng Ä‘á»‹nh nghÄ©a Ä‘Æ°á»£c pháº§n thÆ°á»Ÿng, thÃ¬ trong trÆ°á»ng há»£p cá»§a LLM, hÃ m pháº§n thÆ°á»Ÿng láº¡i phá»©c táº¡p hÆ¡n vÃ¬ nÃ³ liÃªn quan cháº·t cháº½ Ä‘áº¿n cÃ¡c yáº¿u tá»‘ cáº£m xÃºc.

* **Human Feedbacks**: ÄÃ¡nh giÃ¡ cá»§a con ngÆ°á»i vá» cÃ¡c káº¿t quáº£ Ä‘Æ°á»£c táº¡o ra bá»Ÿi mÃ´ hÃ¬nh chÃ­nh lÃ  cÆ¡ sá»Ÿ cho quÃ¡ trÃ¬nh nÃ y, vÃ  nhá»¯ng Ä‘Ã¡nh giÃ¡ nÃ y Ä‘Æ°á»£c quy Ä‘á»‹nh báº±ng cÃ¡ch lÆ°á»£ng hoÃ¡ thÃ nh cÃ¡c Ä‘iá»ƒm cá»¥ thá»ƒ. CÃ¡c Ä‘iá»ƒm Ä‘Ã¡nh giÃ¡ nÃ y sau Ä‘Ã³ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh tÃ­nh toÃ¡n pháº§n thÆ°á»Ÿng Ä‘Æ°á»£c gá»i lÃ  Reward Model. ÄÃ¡ng chÃº Ã½ lÃ  Reward Model Ä‘Ã³ng vai trÃ² quan trá»ng nháº¥t trong ká»¹ thuáº­t RLHF, vÃ  Meta Ä‘Ã£ Ä‘áº§u tÆ° ráº¥t nhiá»u ná»— lá»±c Ä‘á»ƒ xÃ¢y dá»±ng má»™t Reward Model Ä‘á»§ cháº¥t lÆ°á»£ng.

Sau khi cÃ³ má»™t reward model, nÃ³ cÅ©ng lÃ  má»™t máº¡ng nÆ¡ ron thÃ¬ ngÆ°á»i ta sá»­ dá»¥ng cÃ¡c chiáº¿n lÆ°á»£c huáº¥n luyá»‡n RLHF phá»• biáº¿n Ä‘á»ƒ huáº¥n luyá»‡n lÃ  **PPO** vÃ  **Rejection Sampling**. Trong quÃ¡ trÃ¬nh fine-tuning, reward model sáº½ Ä‘Æ°á»£c cáº­p nháº­t liÃªn tá»¥c vá»›i dá»¯ liá»‡u má»›i. RLHF giÃºp mÃ´ hÃ¬nh tiáº¿p cáº­n gáº§n hÆ¡n vá»›i phÃ¢n phá»‘i thá»±c táº¿. áº¢nh dÆ°á»›i mÃ´ táº£ sá»± chuyá»ƒn Ä‘á»•i tá»« phÃ¢n phá»‘i cá»§a mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i Supervised Fine-tuning sang phÃ¢n phá»‘i cá»§a RLHF. PhÃ¢n phá»‘i RLHF hiá»ƒn thá»‹ sá»± gáº§n gÅ©i hÆ¡n vá»›i con ngÆ°á»i.

![](/static/images/GPT-to-LLaMA-2/rlhf.jpg)

## CÃ¡ch thá»©c huáº¥n luyá»‡n

NhÆ° Ä‘Ã£ Ä‘á» cáº­p á»Ÿ pháº§n trÆ°á»›c, viá»‡c huáº¥n luyá»‡n má»™t reward model hiá»‡u quáº£ cÃ³ thá»ƒ Ä‘Æ°á»£c coi lÃ  yáº¿u tá»‘ then chá»‘t quyáº¿t Ä‘á»‹nh sá»± thÃ nh báº¡i cá»§a ká»¹ thuáº­t RLHF. Trong bÃ i bÃ¡o nÃ y, tÃ¡c giáº£ Ä‘Ã£ trÃ¬nh bÃ y chi tiáº¿t vá» quÃ¡ trÃ¬nh xÃ¢y dá»±ng reward model. Há» Ä‘Ã£ phÃ¡t triá»ƒn hai reward model riÃªng biá»‡t, vá»›i cÃ¡c tÃ­nh cháº¥t sau:

* Sá»­ dá»¥ng hai reward model Ä‘á»™c láº­p cho hai khÃ­a cáº¡nh khÃ¡c nhau: Ä‘á»™ an toÃ n (safety) vÃ  Ä‘á»™ há»¯u Ã­ch (helpfulness).
* Sá»­ dá»¥ng scaling law Ä‘á»ƒ Æ°á»›c tÃ­nh sá»‘ lÆ°á»£ng dá»¯ liá»‡u vÃ  tÃ i nguyÃªn cáº§n thiáº¿t cho viá»‡c huáº¥n luyá»‡n reward model.

Äá»ƒ lÃ m rÃµ hÆ¡n vá» hai mÃ´ hÃ¬nh nÃ y, bÃ i bÃ¡o Ä‘Ã£ mÃ´ táº£ ráº±ng reward model Ä‘Æ°á»£c phÃ¢n chia thÃ nh hai loáº¡i, má»™t loáº¡i tá»‘i Æ°u cho tÃ­nh an toÃ n vÃ  má»™t loáº¡i Ä‘Æ°á»£c tá»‘i Æ°u cho tÃ­nh há»¯u Ã­ch. Cáº£ hai mÃ´ hÃ¬nh nÃ y Ä‘á»u dá»±a trÃªn LLaMA-2-Chat, chá»‰ khÃ¡c biá»‡t lÃ  thay vÃ¬ sá»­ dá»¥ng cÃ¡c head cá»§a mÃ´ hÃ¬nh ngÃ´n ngá»¯ (next-token prediction), chÃºng Ä‘Æ°á»£c thay tháº¿ báº±ng regression head Ä‘á»ƒ táº¡o ra Ä‘áº§u ra dáº¡ng scalar. LÃ½ do cho viá»‡c sá»­ dá»¥ng cÃ¹ng má»™t base model vá»›i Chat model Ä‘Æ°á»£c giáº£i thÃ­ch ráº±ng reward model "biáº¿t" nhá»¯ng gÃ¬ mÃ  chat model biáº¿t, tá»©c lÃ  hai mÃ´ hÃ¬nh nÃ y chia sáº» cÃ¹ng má»™t kiáº¿n trÃºc nÃªn trÃ¡nh Ä‘Æ°á»£c sá»± mÆ¡ há»“ vÃ  khÃ´ng tÆ°Æ¡ng á»©ng trong viá»‡c suy luáº­n. Do Ä‘Ã³, há» sá»­ dá»¥ng cÃ¡c checkpoint gáº§n nháº¥t cá»§a chat model lÃ m base cho reward model.

## Má»™t vÃ i lÆ°u Ã½

Má»™t vÃ i lÆ°u Ã½ vá» máº·t kÄ© thuáº­t mÃ¬nh cÃ³ note láº¡i nhÆ° sau:

* **Tá»•ng há»£p dá»¯ liá»‡u**: Há» sá»­ dá»¥ng cÃ¡c dá»¯ liá»‡u nguá»“n má»Ÿ káº¿t há»£p vá»›i dá»¯ liá»‡u Ä‘Æ°á»£c tá»± gÃ¡n nhÃ£n Ä‘á»ƒ huáº¥n luyá»‡n reward model. Ban Ä‘áº§u, chá»‰ cÃ³ cÃ¡c dá»¯ liá»‡u nguá»“n má»Ÿ Ä‘Æ°á»£c sá»­ dá»¥ng, nhÆ°ng sau Ä‘Ã³ há» nháº­n tháº¥y ráº±ng cÃ¡c dá»¯ liá»‡u nÃ y khÃ´ng áº£nh hÆ°á»Ÿng tiÃªu cá»±c Ä‘áº¿n káº¿t quáº£ cá»§a RLHF, nÃªn há» tiáº¿p tá»¥c giá»¯ láº¡i chÃºng trong cÃ¡c quÃ¡ trÃ¬nh huáº¥n luyá»‡n sau nÃ y.

* **PhÃ¢n chia dá»¯ liá»‡u**: Há» giá»¯ láº¡i 90% dá»¯ liá»‡u tá»« mix harmlessness cá»§a Anthropic vÃ  10% dá»¯ liá»‡u tá»« Meta, tuy nhiÃªn há» khÃ´ng giáº£i thÃ­ch lÃ½ do cÅ©ng nhÆ° táº¡i sao chá»‰ sá»­ dá»¥ng 10% dá»¯ liá»‡u tá»« Meta. CÃ³ thá»ƒ há» giá»¯ láº¡i 90% dá»¯ liá»‡u Ä‘á»ƒ táº¡o ra má»™t model máº¡nh máº½ hÆ¡n, nhÆ°ng Ä‘iá»u nÃ y cáº§n Ä‘Æ°á»£c xÃ¡c nháº­n.

* **Huáº¥n luyá»‡n**: Há» chá»‰ huáº¥n luyá»‡n 1 epoch má»—i láº§n cáº­p nháº­t dá»¯ liá»‡u Ä‘á»ƒ trÃ¡nh viá»‡c quÃ¡ má»©c.

* **ÄÃ¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c**: Äá»™ chÃ­nh xÃ¡c trung bÃ¬nh cá»§a reward model dao Ä‘á»™ng tá»« 60 Ä‘áº¿n 70% theo paper, nhÆ°ng Ä‘á»‘i vá»›i cÃ¡c trÆ°á»ng há»£p "Significantly Better", Ä‘á»™ chÃ­nh xÃ¡c tÄƒng lÃªn Ä‘áº¿n 90%. Äiá»u nÃ y cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c bá»Ÿi vÃ¬ cÃ¡c trÆ°á»ng há»£p nÃ y Ã­t phÃ¢n vÃ¢n hÆ¡n vÃ  dá»… lá»±a chá»n hÆ¡n cho con ngÆ°á»i. Tuy nhiÃªn, Ä‘á»‘i vá»›i cÃ¡c trÆ°á»ng há»£p mÃ  con ngÆ°á»i cÅ©ng khÃ´ng cháº¯c cháº¯n, Ä‘á»™ chÃ­nh xÃ¡c chá»‰ khoáº£ng 50%, tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i viá»‡c lá»±a chá»n ngáº«u nhiÃªn.

* **Huáº¥n luyá»‡n vá»›i ranking loss**: Há» sá»­ dá»¥ng ranking loss vÃ  tham sá»‘ margin Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t há»c cá»§a mÃ´ hÃ¬nh vá»›i cÃ¡c má»©c Ä‘á»™ khÃ¡c nhau cá»§a Ä‘iá»ƒm sá»‘.

$$
\mathcal{L}_{\text{ranking}} = -\log(\sigma(r_{\theta}(x,y_{c}) - r_{\theta}(x,y_{r} - m(r)))
$$

* **ÄÃ¡nh giÃ¡ so sÃ¡nh vá»›i GPT-4**: TÃ¡c giáº£ Ä‘Ã£ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh cá»§a há» vá»›i GPT-4 báº±ng cÃ¡ch sá»­ dá»¥ng zero-shot prompt "Choose the best answer between A and B" vá»›i A vÃ  B lÃ  hai cÃ¢u tráº£ lá»i tÆ°Æ¡ng á»©ng cá»§a mÃ´ hÃ¬nh. Káº¿t quáº£ cho tháº¥y mÃ´ hÃ¬nh reward model cá»§a há» vÆ°á»£t trá»™i hÆ¡n so vá»›i GPT-4, máº·c dÃ¹ chá»‰ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn cÃ¡c táº­p dá»¯ liá»‡u cÃ´ng khai.

Biá»ƒu Ä‘á»“ bÃªn dÆ°á»›i thá»ƒ hiá»‡n má»©c Ä‘á»™ tÄƒng trÆ°á»Ÿng accuracy cá»§a mÃ´ hÃ¬nh reward model khi bá»• sung thÃªm dá»¯ liá»‡u. CÃ¡c Ä‘á»‘i tÃ¡c Ä‘á»ƒ lÃ m dá»¯ liá»‡u cá»§a há» sáº½ gá»­i dá»¯ liá»‡u theo batches hÃ ng tuáº§n

![](/static/images/GPT-to-LLaMA-2/accuracy-rewards.png)

# Tham kháº£o
[BÃ i viáº¿t cá»§a anh ToÃ n](https://viblo.asia/p/tat-tan-tat-ve-LLaMA-2-lieu-co-du-lam-nen-mot-cuoc-cach-mang-moi-GyZJZXe2Vjm)

[Technical report cá»§a LLaMA-2](https://llama.meta.com/llama2/)

