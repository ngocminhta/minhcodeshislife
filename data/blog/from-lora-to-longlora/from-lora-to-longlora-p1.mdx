---
title: 'Tá»« LoRA Ä‘áº¿n LongLoRA: Nháº¹ hÆ¡n, dÃ i hÆ¡n vÃ  sÆ°á»›ng hÆ¡n - Pháº§n 1'
date: '2024-02-05'
lastmod: '2023-02-05'
tags: ['deep learning', 'lora', 'llm', 'fine-tune']
draft: false
summary: 'LoRA Ä‘Ã£ trá»Ÿ thÃ nh má»™t kÄ© thuáº­t fine-tuning Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i vá»›i sá»± tá»‘i Æ°u cá»§a nÃ³. Trong series nÃ y, chÃºng ta sáº½ tháº£o luáº­n vá» LoRA, lÆ°á»£ng tá»­ hoÃ¡ vÃ  má»™t phiÃªn báº£n má»Ÿ rá»™ng cá»§a LoRA vá»›i context dÃ i hÆ¡n.'
images: ['/static/images/from-lora-to-longlora/alpaca.jpg']
---

![](/static/images/from-lora-to-longlora/alpaca.jpg)

> ğŸ’¡ Hilu má»i ngÆ°á»i, xin lá»—i vÃ¬ 2 tuáº§n vá»«a rá»“i mÃ¬nh sá»‘ng cháº¿t vá»›i deadline vÃ  thi cuá»‘i kÃ¬ nÃªn hÃ´m nay mÃ¬nh má»›i quay trá»Ÿ láº¡i vá»›i blog nÃ y. HÃ´m nay, mÃ¬nh sáº½ giá»›i thiá»‡u vá»›i má»i ngÆ°á»i cÃ¡c kÄ© thuáº­t thÃ´ng dá»¥ng Ä‘á»ƒ fine-tuning lÃ  LoRA, QLoRA vÃ  LongLoRA.

ChÃºng ta Ä‘á»u biáº¿t fine-tuning lÃ  má»™t phÆ°Æ¡ng phÃ¡p cá»§a transfer learning, sá»­ dá»¥ng weight cá»§a má»™t pre-trained model Ä‘á»ƒ train vá»›i má»™t bá»™ data má»›i, phÃ¹ há»£p vá»›i má»¥c Ä‘Ã­ch cá»§a ngÆ°á»i dÃ¹ng vÃ  sá»‘ lÆ°á»£ng dataset thÆ°á»ng nhá» hÆ¡n khi pre-train. Viá»‡c lÃ m nÃ y giÃºp tÄƒng Ä‘á»™ chÃ­nh xÃ¡c cá»§a model so vá»›i viá»‡c train trá»±c tiáº¿p vá»›i bá»™ dataset nhá» cá»§a chÃºng ta. ThÃ´ng thÆ°á»ng, khi thá»±c hiá»‡n fine-tuning, ta sáº½ pháº£i train toÃ n bá»™ hoáº·c má»™t sá»‘ layers cá»§a model, vÃ  cÅ©ng pháº£i lÆ°u láº¡i toÃ n bá»™ cÃ¡c tham sá»‘ cá»§a model hoáº·c má»™t sá»‘ layers cá»§a model Ä‘Æ°á»£c fine-tune luÃ´n. Tá»©c lÃ  vá»›i 10 downstream tasks, ta sáº½ pháº£i train toÃ n bá»™ model 10 láº§n, xong láº¡i lÆ°u láº¡i weight cá»§a cáº£ 10 models. Äá»‘i vá»›i nhá»¯ng model nhá» thÃ¬ Ä‘iá»u nÃ y khÃ´ng pháº£i lÃ  má»™t váº¥n Ä‘á» lá»›n, tuy nhiÃªn, trong cÃ¡i ká»‰ nguyÃªn mÃ  ngÆ°á»i ngÆ°á»i nhÃ  nhÃ  sá»­ dá»¥ng cÃ¡c model cá»±c náº·ng, tá»« vÃ i trÄƒm triá»‡u Ä‘áº¿n vÃ i tá»‰ tham sá»‘ thÃ¬ viá»‡c train toÃ n bá»™ model, vÃ  lÆ°u toÃ n bá»™ model lÃ  má»™t váº¥n Ä‘á» cá»±c kÃ¬ khÃ³ khÄƒn vá»›i nhá»¯ng ngÆ°á»i bá»‹ giá»›i háº¡n vá» máº·t pháº§n cá»©ng.

# LoRA lÃ  gÃ¬?

VÃ¬ Ä‘á»™ lá»›n cá»§a LLM Ä‘ang tÄƒng lÃªn má»™t cÃ¡ch chÆ°a tá»«ng cÃ³ nÃªn chÃºng ta sáº½ cá»‘ gáº¯ng khÃ´ng Ä‘á» cáº­p Ä‘áº¿n cÃ¡c tham sá»‘ Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n mÃ  sáº½ thÃªm cÃ¡c layer vÃ o LLM hoáº·c thÃªm giÃ¡ trá»‹ vÃ o cÃ¡c tham sá»‘. CÃ¡c layer Ä‘Æ°á»£c thÃªm vÃ o thÆ°á»ng Ä‘Æ°á»£c gá»i lÃ  â€œadaptersâ€ vÃ  ká»¹ thuáº­t fine-tuning Ä‘Æ°á»£c gá»i lÃ  â€œadapter-tuningâ€. NÃ³ liÃªn quan Ä‘áº¿n viá»‡c thÃªm cÃ¡c mÃ´-Ä‘un adapter nhá» vÃ o mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c vÃ  chá»‰ huáº¥n luyá»‡n cÃ¡c tham sá»‘ trong mÃ´-Ä‘un adapter.

Tuy nhiÃªn, ngÆ°á»i ta phÃ¡t hiá»‡n ra ráº±ng cÃ¡c layer bá»• sung gÃ¢y ra Ä‘á»™ trá»… trong quÃ¡ trÃ¬nh dá»± Ä‘oÃ¡n, Ä‘Æ°á»£c gá»i lÃ  **Äá»™ trá»… Inference**. Sáº½ khÃ´ng dá»… chá»‹u náº¿u báº¡n pháº£i Ä‘á»£i hÆ¡n 20 giÃ¢y Ä‘á»ƒ LLM Ä‘Æ°a ra cÃ¢u tráº£ lá»i. Sá»± cá»‘ nÃ y dÆ°á»ng nhÆ° khÃ´ng thá»ƒ trÃ¡nh khá»i vÃ¬ cÃ¡c adapter layer Ä‘Æ°á»£c thÃªm tuáº§n tá»± vÃ o LLM, chÃºng pháº£i Ä‘Æ°á»£c xá»­ lÃ½ tuáº§n tá»± vÃ  khÃ´ng cÃ³ cÃ¡ch nÃ o Ä‘á»ƒ xá»­ lÃ½ song song. Báº¡n cÃ³ thá»ƒ sáº½ tháº¯c máº¯c táº¡i sao khÃ´ng sá»­ dá»¥ng batching Ä‘á»ƒ chia dá»¯ liá»‡u nháº±m Ä‘áº¡t tá»‘c Ä‘á»™ nhanh hÆ¡n. Ã kiáº¿n â€‹â€‹Ä‘Ã³ hay Ä‘áº¥y. NhÆ°ng trong quÃ¡ trÃ¬nh sá»­ dá»¥ng hoáº·c suy luáº­n theo thá»i gian thá»±c, ngÆ°á»i dÃ¹ng thÆ°á»ng nháº­p láº§n lÆ°á»£t tá»«ng prompt hoáº·c cÃ¢u há»i, do Ä‘Ã³ batch size lÃ  1 vÃ  khÃ´ng cÃ³ nhiá»u dá»¯ liá»‡u Ä‘Æ°á»£c batching.

LoRA (**Lo**w-**R**ank **A**daptation of Large Language Models) khÃ´ng thÃªm layer mÃ  thÃªm giÃ¡ trá»‹ vÃ o tham sá»‘. Giáº£i phÃ¡p nÃ y sáº½ khÃ´ng gÃ¢y ra Ä‘á»™ trá»… inference. Ta sáº½ sá»›m Ä‘i sÃ¢u vÃ o kiáº¿n trÃºc cá»§a LoRA, nhÆ°ng táº¡i thá»i Ä‘iá»ƒm nÃ y, táº¥t cáº£ nhá»¯ng gÃ¬ báº¡n cáº§n nhá»› lÃ : cÃ³ cÃ¡c chiáº¿n lÆ°á»£c fine-tuning khÃ¡c nhau vÃ  chiáº¿n lÆ°á»£c adapt-tuning cÃ³ thá»ƒ gÃ¢y ra Ä‘á»™ trá»… inference. BÃ¢y giá» chÃºng ta Ä‘Ã£ biáº¿t *adaptation* cÃ³ nghÄ©a lÃ  fine-tuning dá»¯ liá»‡u vÃ  tÃ¡c vá»¥ miá»n. NÃ³ khÃ´ng Ä‘Æ°á»£c gá»i lÃ  *adapter* vÃ¬ nÃ³ khÃ´ng thÃªm adapter. NÃ³ Ä‘Æ°á»£c gá»i lÃ  *adaption* Ä‘á»ƒ mÃ´ táº£ quÃ¡ trÃ¬nh thÃ­ch á»©ng cá»§a nÃ³.

# Äá»‹nh nghÄ©a rank trong low-rank
Äá»‹nh nghÄ©a vá» rank cá»§a ma tráº­n lÃ  sá»‘ cá»™t Ä‘á»™c láº­p tuyáº¿n tÃ­nh tá»‘i Ä‘a cá»§a ma tráº­n. DÆ°á»›i Ä‘Ã¢y, chÃºng ta thá»ƒ hiá»‡n hiá»ƒn thá»‹ ma tráº­n 2x3 vÃ  ma tráº­n chuyá»ƒn vá»‹ cá»§a nÃ³ vá»›i chiá»u 3x2. LÆ°u Ã½ ráº±ng hai hÃ ng cá»§a ma tráº­n hoÃ n toÃ n Ä‘á»‘i nhau. ChÃºng khÃ´ng Ä‘á»™c láº­p tuyáº¿n tÃ­nh. HÃ ng Ä‘á»™c láº­p tuyáº¿n tÃ­nh duy nháº¥t lÃ  hÃ ng Ä‘áº§u tiÃªn. VÃ¬ váº­y, cáº¥p cá»§a V lÃ  1. TÆ°Æ¡ng tá»± nhÆ° váº­y, cáº¥p chuyá»ƒn vá»‹ cá»§a nÃ³ cÅ©ng lÃ  1.

<div className="math-block" data-math="V = \begin{bmatrix}
1 & 2 & 1 \\
-1 & -2 & -1
\end{bmatrix} \ ; \quad
V^{T} = \begin{bmatrix}
1 & -1 \\
2 & -2 \\
1 & -1
\end{bmatrix}"></div>

CÃ¡c hÃ ng hoáº·c cá»™t bá»• sung khÃ´ng cÃ³ Ä‘á»™c láº­p tuyáº¿n tÃ­nh vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c xÃ¢y dá»±ng láº¡i bá»Ÿi cÃ¡c hÃ ng hoáº·c cá»™t khÃ¡c. Vá»›i thuá»™c tÃ­nh nÃ y, máº·c dÃ¹ ma tráº­n cÃ³ thá»ƒ ráº¥t lá»›n nhÆ°ng thÃ´ng tin thá»±c sá»± cá»§a nÃ³ chá»‰ náº±m trÃªn cÃ¡c hÃ ng hoáº·c cá»™t Ä‘á»™c láº­p.

# QuÃ¡ trÃ¬nh fine-tuning

![QuÃ¡ trÃ¬nh cáº­p nháº­t má»™t layer cá»§a model trong fine-tuning](/static/images/from-lora-to-longlora/lora-finetune.png)

Pre-trained weights <span className="math-inline" data-math="W"></span> cá»§a model sáº½ biáº¿n thÃ nh updated weights <span className="math-inline" data-math="W'"></span> dá»±a trÃªn giÃ¡ trá»‹ weight cáº§n thay Ä‘á»•i <span className="math-inline" data-math="\Delta W"></span> thu Ä‘Æ°á»£c tá»« quÃ¡ trÃ¬nh back propagation. VÃ  á»Ÿ iteration tiáº¿p theo, <span className="math-inline" data-math="W'"></span> láº¡i Ä‘Æ°á»£c update vá»›i má»™t <span className="math-inline" data-math="\Delta W"></span> khÃ¡c.

QuÃ¡ trÃ¬nh forward sau má»—i iteration nhÆ° sau:
* Iteration 0: <span className="math-inline" data-math="y = Wx"></span>
* Iteration 1: <span className="math-inline" data-math="y = W'x = (W + \Delta W)x"></span>
* Iteration 2: <span className="math-inline" data-math="y = W''x = (W + \Delta W)x = (W' + \Delta W')x = (W + \Delta W + \Delta W')x"></span>

![](/static/images/from-lora-to-longlora/update-w.png)

## Má»¥c tiÃªu cá»§a LoRA

Má»¥c Ä‘Ã­ch cá»§a LoRA lÃ  tÃ¬m cÃ¡ch biá»ƒu diá»…n ma tráº­n $\Delta W$ thÃ nh má»™t dáº¡ng biá»ƒu diá»…n nháº¹ hÆ¡n. NÄƒm 2020, cÃ³ má»™t paper nÃ³i ráº±ng nhá»¯ng mÃ´ hÃ¬nh ngÃ´n ngá»¯ pre-trained cÃ³ intrinsic dimension (hay intrinsic rank) cá»±c kÃ¬ tháº¥p, tá»©c lÃ , model nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n sá»­ dá»¥ng sá»‘ chiá»u Ã­t hÆ¡n ráº¥t nhiá»u so vá»›i sá»‘ chiá»u gá»‘c cá»§a model, mÃ  váº«n giá»¯ Ä‘Æ°á»£c performance khi Ä‘em Ä‘i fine-tune.

Táº­n dá»¥ng Ã½ tÆ°á»Ÿng nÃ y, nhÃ³m tÃ¡c giáº£ cá»§a LoRA cÅ©ng cho ráº±ng, <span className="math-inline" data-math="\Delta W"></span> cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n vá»›i sá»‘ chiá»u Ã­t hÆ¡n ráº¥t nhiá»u sá»‘ chiá»u gá»‘c cá»§a <span className="math-inline" data-math="\Delta W"></span>. LoRA chá»n sá»­ dá»¥ng Matrix decomposition Ä‘á»ƒ biá»ƒu diá»…n ma tráº­n <span className="math-inline" data-math="\Delta W"></span> báº±ng tÃ­ch cá»§a cÃ¡c ma tráº­n con vá»›i Ä‘á»™ náº·ng tÃ­nh toÃ¡n tháº¥p hÆ¡n viá»‡c tÃ­nh trÃªn ma tráº­n gá»‘c. CÃ³ ráº¥t nhiá»u phÆ°Æ¡ng phÃ¡p Matrix decomposition (LU decomposition, Singular Value Decomposition, ...), vÃ  LoRA chá»n sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p Neural Network.

LoRA phÃ¢n rÃ£ <span className="math-inline" data-math="\Delta W"></span> thÃ nh 2 ma tráº­n con <span className="math-inline" data-math="A"></span> vÃ  <span className="math-inline" data-math="B"></span> vá»›i rank tháº¥p hÆ¡n ráº¥t nhiá»u so vá»›i ma tráº­n ban Ä‘áº§u. Cá»¥ thá»ƒ, <span className="math-inline" data-math="\Delta W = BA"></span> vá»›i <span className="math-inline" data-math="\Delta W \in \mathrm{R}^{d \times k}, B \in \mathrm{R}^{d \times r}, A \in \mathrm{R}^{r \times k}"></span> vÃ  sá»‘ rank <span className="math-inline" data-math="r \ll \min (d,k)"></span>. LÃºc nÃ y, output cá»§a layer Ä‘Ã³ sáº½ trá»Ÿ thÃ nh

<div className="math-block" data-math="y = W_{0}x + \Delta Wx = W_{0}x + BAx"></div>

Ma tráº­n A Ä‘Æ°á»£c khá»Ÿi táº¡o theo Gaussian initialization, cÃ²n B thÃ¬ Ä‘Æ°á»£c khá»Ÿi táº¡o lÃ  ma tráº­n 0, nÃªn <span className="math-inline" data-math="\Delta W = BA"></span> cÃ³ giÃ¡ trá»‹ báº±ng 0 khi báº¯t Ä‘áº§u training. QuÃ¡ trÃ¬nh training sáº½ tá»‘i Æ°u Ä‘Ãª tÃ­nh Ä‘Æ°á»£c 2 ma tráº­n cáº§n tÃ¬m.

Váº­y táº¡i sao viá»‡c phÃ¢n tÃ¡ch nÃ y láº¡i lÃ m khá»‘i lÆ°á»£ng tÃ­nh toÃ¡n nháº¹ Ä‘i? Giáº£ sá»­ ta chá»n <span className="math-inline" data-math="A = B = 100, r = 4"></span>. Khi Ä‘Ã³, ta cÃ³ sá»‘ pháº§n tá»­ cá»§a ma tráº­n <span className="math-inline" data-math="\Delta W"></span> lÃ  <span className="math-inline" data-math="100 \times 100 = 10000"></span>, cÃ²n sá»‘ phÃ¢n tá»­ sau phÃ¢n rÃ£ lÃ  <span className="math-inline" data-math="100 \times 4 + 100 \times 4 = 800"></span>, tá»©c lÃ  khá»‘i lÆ°á»£ng Ä‘Æ°á»£c giáº£m Ä‘i 12.5 láº§n.

Äá»ƒ cÃ³ thá»ƒ phÃ¢n tÃ¡ch Ä‘Æ°á»£c má»™t layer Linear, LoRA thÃªm vÃ o Linear layer Ä‘Ã³ 2 lá»›p Linear ná»¯a, má»—i lá»›p Linear Ä‘áº¡i diá»‡n cho ma tráº­n <span className="math-inline" data-math="A"></span> vÃ  <span className="math-inline" data-math="B"></span>. LÃºc nÃ y nÃ³ giá»‘ng nhÆ° viá»‡c training má»™t NN bÃ¬nh thÆ°á»ng thÃ´i. ChÃº Ã½ lÃ , hiá»‡n táº¡i LoRA má»›i chá»‰ há»— trá»£ phÃ¢n tÃ¡ch weight cá»§a layer Linear, vÃ  chÆ°a há»— trá»£ cho nhá»¯ng layer khÃ¡c.

![](/static/images/from-lora-to-longlora/decompose.png)

## Æ¯u Ä‘iá»ƒm cá»§a LoRA

BÃªn cáº¡nh viá»‡c khÃ´ng cÃ³ Ä‘á»™ trá»… inference nhÆ° Ä‘Ã£ giáº£i thÃ­ch á»Ÿ trÃªn, hÃ£y cÃ¹ng xem xÃ©t thÃªm cÃ¡c Æ°u Ä‘iá»ƒm cá»§a LoRA.

Äáº§u tiÃªn lÃ  giáº£m sá»‘ lÆ°á»£ng tham sá»‘ huáº¥n luyá»‡n. CÃ¡c tÃ¡c giáº£ cá»§a LoRA Ä‘Ã£ chá»©ng minh Ä‘Æ°á»£c, so vá»›i GPT-3 175B Ä‘Æ°á»£c tinh chá»‰nh vá»›i Adam, LoRA cÃ³ thá»ƒ giáº£m sá»‘ lÆ°á»£ng tham sá»‘ training 10.000 láº§n vÃ  giáº£m yÃªu cáº§u bá»™ nhá»› GPU Ä‘i 3 láº§n. Má»™t lá»£i tháº¿ liÃªn quan lÃ  thiáº¿t káº¿ cáº¥p tháº¥p cá»§a nÃ³. Bá»Ÿi vÃ¬ nÃ³ chá»‰ tá»‘i Æ°u hÃ³a cÃ¡c ma tráº­n cáº¥p tháº¥p nÃªn mang láº¡i hiá»‡u quáº£ cho viá»‡c huáº¥n luyá»‡n mÃ´ hÃ¬nh.

Æ¯u Ä‘iá»ƒm tiáº¿p theo lÃ  tÃ­nh mÃ´-Ä‘un. Báº¡n cÃ³ thá»ƒ xÃ¢y dá»±ng nhiá»u mÃ´-Ä‘un LoRA nhá» cho cÃ¡c nhiá»‡m vá»¥ khÃ¡c nhau. Æ¯u Ä‘iá»ƒm cá»§a tÃ­nh mÃ´-Ä‘un nÃ y tÆ°Æ¡ng tá»± nhÆ° lá»£i tháº¿ cá»§a viá»‡c Ä‘iá»u chá»‰nh tiá»n tá»‘. VÃ­ dá»¥: báº¡n cÃ³ thá»ƒ xÃ¢y dá»±ng mÃ´-Ä‘un LoRA trÃªn LLM cÆ¡ sá»Ÿ Ä‘á»ƒ tÃ³m táº¯t vÄƒn báº£n vÃ  má»™t mÃ´-Ä‘un LoRA khÃ¡c trÃªn cÃ¹ng LLM cÆ¡ sá»Ÿ cho cÃ¢u há»i vÃ  cÃ¢u tráº£ lá»i. Khi hai mÃ´ hÃ¬nh tinh chá»‰nh Ä‘Æ°á»£c triá»ƒn khai Ä‘á»ƒ suy luáº­n theo thá»i gian thá»±c, báº¡n chá»‰ cáº§n táº£i cÃ¹ng má»™t mÃ´ hÃ¬nh cÆ¡ sá»Ÿ má»™t láº§n. Vá»›i kÃ­ch thÆ°á»›c váº­t lÃ½ cá»§a LLM á»Ÿ má»©c hÆ¡n 100 GB, khÃ´ng thá»ƒ bá» qua lá»£i tháº¿ nÃ y.

## QuÃ¡ tÃ¬nh fine-tuning váº«n tiÃªu tá»‘n nhiá»u GPU

Tuy chÃºng ta hÃ i lÃ²ng vá»›i chiáº¿n lÆ°á»£c khÃ´ng cháº¡m tá»›i hÃ ng tá»· tham sá»‘ trong mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c nhÆ°ng cÃ¡c thÃ¡ch thá»©c váº«n chÆ°a káº¿t thÃºc. Viá»‡c tinh chá»‰nh LLM cÅ©ng cáº§n nhiá»u GPU. NhÆ° má»™t bÃ i viáº¿t Ä‘Ã£ nÃ³i, chá»‰ Ä‘á»ƒ sá»­ dá»¥ng (inference) mÃ´ hÃ¬nh BLOOM-176B sáº½ yÃªu cáº§u GPU A100 8x 80GB. Náº¿u chÃºng ta muá»‘n tinh chá»‰nh BLOOM-176B, chÃºng ta sáº½ cáº§n 72 GPU nÃ y. CÃ¡c mÃ´ hÃ¬nh lá»›n hÆ¡n nhiá»u nhÆ° PaLM tháº­m chÃ­ cÃ²n Ä‘Ã²i há»i nhiá»u tÃ i nguyÃªn hÆ¡n.

CÃ³ ráº¥t nhiá»u ná»— lá»±c chung nháº±m giáº£m viá»‡c sá»­ dá»¥ng GPU trong suy luáº­n vÃ  tinh chá»‰nh mÃ´ hÃ¬nh. Má»™t bÆ°á»›c phÃ¡t triá»ƒn quan trá»ng lÃ  int8 cÃ³ thá»ƒ giáº£m má»™t ná»­a yÃªu cáº§u bá»™ nhá»› mÃ  khÃ´ng lÃ m giáº£m hiá»‡u nÄƒng cá»§a mÃ´ hÃ¬nh.

## CÃ¡c kÄ© thuáº­t Ä‘Ãª giáº£m tÃ i nguyÃªn GPU

ChÃºng ta Ä‘á»u biáº¿t kÃ­ch thÆ°á»›c cá»§a má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh bá»Ÿi sá»‘ lÆ°á»£ng tham sá»‘ cá»§a nÃ³, thá»±c táº¿ ráº±ng Ä‘á»™ chÃ­nh xÃ¡c cá»§a má»™t tham sá»‘ cÅ©ng quyáº¿t Ä‘á»‹nh ráº¥t lá»›n Ä‘áº¿n kÃ­ch thÆ°á»›c cá»§a mÃ´ hÃ¬nh. Má»™t tham sá»‘ thÆ°á»ng Ä‘Æ°á»£c lÆ°u trá»¯ vÃ  trÃ¬nh bÃ y trong float32. Float32 (FP32) lÃ  viáº¿t táº¯t cá»§a biá»ƒu diá»…n dáº¥u pháº©y Ä‘á»™ng 32-bit IEEE Ä‘Æ°á»£c tiÃªu chuáº©n hÃ³a. HÃ¬nh dÆ°á»›i biá»ƒu diá»…n dáº¥u pháº©y Ä‘á»™ng 32 bit cho giÃ¡ trá»‹ 0.15625. Biá»ƒu diá»…n 32 bit cho táº¥t cáº£ cÃ¡c tham sá»‘ lÃ m cho kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh ráº¥t lá»›n.

![](/static/images/from-lora-to-longlora/fp32.jpg)

ChÃºng ta cÃ³ thá»ƒ lÃ m gÃ¬? Cá»™ng Ä‘á»“ng nghiÃªn cá»©u Ä‘Ã£ Ä‘Æ°a ra má»™t Ä‘á»‹nh dáº¡ng má»›i, bfloat16 (BF16). Äá»‹nh dáº¡ng BF16 yÃªu cáº§u 2 byte so vá»›i Ä‘á»‹nh dáº¡ng FP32 yÃªu cáº§u 4 byte. VÃ¬ lÃ½ do nÃ y, Ä‘á»‹nh dáº¡ng FP32 Ä‘Æ°á»£c gá»i lÃ  Ä‘á»™ chÃ­nh xÃ¡c Ä‘áº§y Ä‘á»§ (4 byte) vÃ  Ä‘á»‹nh dáº¡ng BF16 cÃ³ Ä‘á»™ chÃ­nh xÃ¡c má»™t ná»­a (2 byte).

LÃ m cÃ¡ch nÃ o chÃºng ta cÃ³ thá»ƒ duy trÃ¬ Ä‘á»™ chÃ­nh xÃ¡c cá»§a cÃ¡c tham sá»‘ vÃ  giáº£m yÃªu cáº§u vá» GPU? Chiáº¿n lÆ°á»£c lÃ  Ã¡p dá»¥ng cÃ¡ch tiáº¿p cáº­n cÃ³ Ä‘á»™ chÃ­nh xÃ¡c há»—n há»£p. NÃ³ giá»¯ cÃ¡c tham sá»‘ cá»§a mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c trong FP32 lÃ m trá»ng sá»‘ chÃ­nh trong khi thá»±c hiá»‡n tÃ­nh toÃ¡n trong BP16. Trong quÃ¡ trÃ¬nh tinh chá»‰nh, viá»‡c tÃ­nh toÃ¡n Ä‘á»™ dá»‘c Ä‘Æ°á»£c thá»±c hiá»‡n trong FP16, sau Ä‘Ã³ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ cáº­p nháº­t cÃ¡c tham sá»‘ chÃ­nh.

Tuy nhiÃªn, viá»‡c sá»­ dá»¥ng BP16 vá»›i Ä‘á»‹nh dáº¡ng FP32 váº«n khÃ´ng lÃ m giáº£m kÃ­ch thÆ°á»›c cá»§a mÃ´ hÃ¬nh xuá»‘ng má»©c dá»… quáº£n lÃ½ hÆ¡n. Äá»ƒ kháº¯c phá»¥c, Tim Dettmers vÃ  Ä‘á»“ng bá»n giá»›i thiá»‡u phÆ°Æ¡ng phÃ¡p lÆ°á»£ng tá»­ hÃ³a 8 bit (int8 quantization). Bá»Ÿi vÃ¬ nÃ³ lÃ  8 bit, tá»©c 1/4 cá»§a 32 bit, nÃªn nÃ³ cÃ³ kháº£ nÄƒng giáº£m kÃ­ch thÆ°á»›c cá»§a mÃ´ hÃ¬nh xuá»‘ng cÃ²n 1/4. RÃµ rÃ ng, Ä‘iá»u nÃ y khÃ´ng dá»… dÃ ng nhÆ° váº­y, nÃ³ sáº½ gÃ¢y ra sá»± xuá»‘ng cáº¥p cá»§a mÃ´ hÃ¬nh, viá»‡c dá»± Ä‘oÃ¡n mÃ´ hÃ¬nh cÅ©ng sáº½ bá»‹ áº£nh hÆ°á»Ÿng. Biá»‡n phÃ¡p kháº¯c phá»¥c lÃ  gÃ¬? Há» phÃ¡t hiá»‡n ra ráº±ng trong phÃ©p tÃ­nh nhÃ¢n ma tráº­n, cÃ¡c giÃ¡ trá»‹ outlier quan trá»ng hÆ¡n cÃ¡c giÃ¡ trá»‹ non-outlier. VÃ¬ váº­y, cÃ¡c giÃ¡ trá»‹ outlier cÃ³ thá»ƒ Ä‘Æ°á»£c lÆ°u trá»¯ trong FP16 trong khi cÃ¡c giÃ¡ trá»‹ non-outlier á»Ÿ Ä‘á»‹nh dáº¡ng 8 bit hoáº·c Ä‘Æ°á»£c gá»i lÃ  int8. Sau Ä‘Ã³, cÃ¡c giÃ¡ trá»‹ outlier vÃ  non-outlier Ä‘Æ°á»£c ghÃ©p vá»›i nhau Ä‘á»ƒ nháº­n káº¿t quáº£ Ä‘áº§y Ä‘á»§ trong FP16. QuÃ¡ trÃ¬nh nÃ y Ä‘Ã£ Ä‘Æ°á»£c phÃ¡t triá»ƒn trong thÆ° viá»‡n bitandbytes.

ÄÃ¢y lÃ  tiá»n Ä‘á» cho viá»‡c sÃ¡ng táº¡o ra phÆ°Æ¡ng phÃ¡p LoRA with Quantization (QLoRA), vÃ  chÃºng ta sáº½ tháº£o luáº­n kÄ© hÆ¡n vá» nÃ³ á»Ÿ bÃ i viáº¿t sau, cÃ¹ng vá»›i LongLoRA nhÃ©. ChÃ o thÃ¢n Ã¡i vÃ  quyáº¿t tháº¯ng!

# Tham kháº£o

[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)

[LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/abs/2208.07339)