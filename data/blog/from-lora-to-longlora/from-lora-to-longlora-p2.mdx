---
title: 'Tá»« LoRA Ä‘áº¿n LongLoRA: Nháº¹ hÆ¡n, dÃ i hÆ¡n vÃ  sÆ°á»›ng hÆ¡n - Pháº§n 2'
date: '2024-02-08'
lastmod: '2023-02-08'
tags: ['deep learning', 'lora', 'llm', 'fine-tune']
draft: false
summary: 'á» trong bÃ i nÃ y, chÃºng ta cÃ¹ng tháº£o luáº­n kÄ© hÆ¡n vá» QLoRA vÃ  LongLoRA - hai kÄ© thuáº­t Ä‘Æ°á»£c phÃ¡t triá»ƒn tá»« LoRA vá»›i cÃ¡c má»¥c Ä‘Ã­ch tá»‘i Æ°u hÆ¡n cho tá»«ng má»¥c Ä‘Ã­ch'
images: ['/static/images/from-lora-to-longlora/qlora.png']
---

![](/static/images/from-lora-to-longlora/qlora.png)

> ğŸ’¡ Má»i ngÆ°á»i cÃ³ thá»ƒ Ä‘á»c bÃ i viáº¿t pháº§n 1 cá»§a mÃ¬nh vá» LoRA táº¡i [Ä‘Ã¢y](https://blog.ngocminhta.id.vn/blog/from-lora-to-longlora/from-lora-to-longlora-p1) Ä‘á»ƒ hiá»ƒu rÃµ hÆ¡n trÆ°á»›c khi Ä‘á»c bÃ i viáº¿t nÃ y nha.

ChÃºng ta Ä‘á»u biáº¿t LoRA Ä‘Æ°á»£c sá»­ dá»¥ng nhÆ° má»™t phÆ°Æ¡ng phÃ¡p tá»‘i Æ°u viá»‡c fine-tuning mÃ´ hÃ¬nh báº±ng cÃ¡ch Ä‘Ã³ng bÄƒng toÃ n bá»™ tham sá»‘ vÃ  chá»‰ huáº¥n luyá»‡n trÃªn má»™t pháº§n nhá» (cá»¡ 15 - 20%) cá»§a toÃ n bá»™ model Ä‘á»ƒ cÃ³ thá»ƒ huáº¥n luyá»‡n trÃªn cÃ¡c mÃ¡y tÃ­nh cÃ³ cáº¥u hÃ¬nh háº¡n cháº¿. Váº­y thÃ¬, QLoRA lÃ  gÃ¬, LongLoRA lÃ  gÃ¬ vÃ  táº¡i sao láº¡i cáº§n Ä‘áº¿n hai kÄ© thuáº­t nÃ y?

# QLoRA lÃ  gÃ¬?

Äá»ƒ hiá»ƒu Ä‘Æ¡n giáº£n, QLoRA = Quantization + LoRA, mÃ  kÄ© thuáº­t quantization (lÆ°á»£ng tá»­ hÃ³a) mÃ¬nh Ä‘Ã£ giá»›i thiá»‡u á»Ÿ pháº§n 1 cá»§a bÃ i viáº¿t nÃ y rá»“i. Vá»›i sá»± cÃ³ máº·t cá»§a QLoRA, mÃ¬nh Ä‘Ã£ cÃ³ thá»ƒ fine-tune mÃ´ hÃ¬nh 7 tá»·, 13 tá»· vÃ  33 tá»· tham sá»‘ vá»›i 2 con RTX 3090. ÄÃ¡ng nÃ³i lÃ  mÃ´ hÃ¬nh 7 tá»· tham sá»‘ chá»‰ chiáº¿m 4-5GB/GPU, tá»©c lÃ  ~10GB, hoÃ n toÃ n cÃ³ thá»ƒ training trÃªn Google Colab.

## CÃ¡ch lÆ°á»£ng tá»­ hÃ³a trong QLoRA
Sau khi xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c kiá»ƒu dá»¯ liá»‡u (dtype) cáº§n sá»­ dá»¥ng, bÆ°á»›c tiáº¿p theo lÃ  Ã¡p dá»¥ng cÃ´ng thá»©c Ä‘á»ƒ chuyá»ƒn Ä‘á»•i tá»« dtype gá»‘c (source dtype) sang dtype mÃ  chÃºng ta muá»‘n quantize (target dtype). Äá»ƒ Ä‘áº£m báº£o ráº±ng chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng toÃ n bá»™ giÃ¡ trá»‹ cá»§a target dtype, chÃºng ta sáº½ tá»‰ lá»‡ hÃ³a source dtype vá» khoáº£ng target dtype báº±ng cÃ¡ch chuáº©n hÃ³a vá»›i giÃ¡ trá»‹ tuyá»‡t Ä‘á»‘i lá»›n nháº¥t hiá»‡n cÃ³ cá»§a source dtype. VÃ­ dá»¥, quÃ¡ trÃ¬nh quantize tá»« FP32 vá» INT8 vá»›i khoáº£ng giÃ¡ trá»‹ lÃ  <span className="math-inline" data-math="[-127,127]"></span> sáº½ Ä‘Æ°á»£c thá»±c hiá»‡n nhÆ° sau:

<div className="math-block" data-math="X^{\text{int8}} = \text{round}\left(\dfrac{127}{\text{absmax}\left(X^{\text{FP32}}\right)}X^{\text{FP32}} \right) = \text{round}\left(c^{\text{FP32}} \cdot X^{\text{FP32}} \right)"></div>

vá»›i <span className="math-inline" data-math="c"></span> lÃ  háº±ng sá»‘ quantize, <span className="math-inline" data-math="\text{round}"></span> lÃ  phÃ©p lÃ m trÃ²n.

VÃ­ dá»¥, ta cÃ³ tensor <span className="math-inline" data-math="[0.1,0.2,0.4]"></span> á»Ÿ dáº¡ng FP32 muá»‘n quantize vá» INT8, thÃ¬ háº±ng sá»‘ quantize <span className="math-inline" data-math="c = \dfrac{127}{0.4} = 317.5"></span> vÃ  tensor má»›i á»Ÿ dáº¡ng INT8 lÃ  <span className="math-inline" data-math="[32,64,127]"></span>.

Äá»ƒ Ä‘áº£o ngÆ°á»£c tá»« target dtype vá» source dtype, ta cÃ³ quÃ¡ trÃ¬nh de-quantize:
<div className="math-block" data-math="\text{dequant}\left(c^{\text{FP32}} \cdot X^{\text{FP32}} \right) = \dfrac{X^{\text{int8}}}{c^{\text{FP32}}} = X^{\text{FP32}}"></div>

# QLoRA Ä‘Ã£ lÃ m gÃ¬ Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u quáº£ Ä‘Ã³?

QLoRA: Quantized LoRA lÃ  má»™t paper vá» quantization káº¿t há»£p vs LoRA Ä‘á»ƒ giÃºp training cÃ¡c mÃ´ hÃ¬nh siÃªu náº·ng má»™t cÃ¡ch dá»… dÃ ng. QLoRA giá»›i thiá»‡u ba Ä‘iá»ƒm má»›i:
* NF4 (Normal Float 4): Má»™t dtype má»›i, sá»­ dá»¥ng chá»‰ 4 bit nhÆ°ng Ä‘á»™ chÃ­nh xÃ¡c láº¡i á»Ÿ má»©c cá»±c tá»‘t
* Double Quantization: Quantize 2 láº§n
* Paged Optimizers: TrÃ¡nh lá»—i OOM

ChÃºng ta sáº½ cÃ¹ng nhau Ä‘i tÃ¬m hiá»ƒu cÃ¡c ba Ä‘iá»ƒm má»›i á»Ÿ trÃªn, nhÆ°ng trÆ°á»›c háº¿t, hÃ£y lÆ°u Ã½ ráº±ng: **QLoRA quantize tham sá»‘ vá» NF4, nhÆ°ng sau Ä‘Ã³ pháº£i dequantize lÃªn BF16 Ä‘á»ƒ tÃ­nh toÃ¡n** bá»Ÿi vÃ¬ hiá»‡n táº¡i cÃ¡c GPU chÆ°a há»— trá»£ viá»‡c tÃ­nh toÃ¡n báº±ng dtype NF4.

### NF4 lÃ  gÃ¬, cÃ³ Äƒn Ä‘Æ°á»£c khÃ´ng?

QLoRA sá»­ dá»¥ng kÄ© thuáº­t quantize gá»i lÃ  Quantile Quantization. Thay vÃ¬ chia khoáº£ng giÃ¡ trá»‹ thÃ nh tá»«ng Ä‘oáº¡n báº±ng nhau nhÆ° á»Ÿ Quantize thÃ´ng thÆ°á»ng, thÃ¬ Quantile Quantization sáº½ coi khoáº£ng giÃ¡ trá»‹ nhÆ° má»™t cÃ¡i phÃ¢n phá»‘i, vÃ  chia lÃ m sao cho tá»«ng pháº§n trong phÃ¢n phá»‘i Ä‘áº¥y cÃ³ xÃ¡c suáº¥t xáº£y ra báº±ng nhau. Má»™t khoáº£ng Ä‘Æ°á»£c chia nhÆ° dÆ°á»›i kia sáº½ gá»i lÃ  má»™t quantile.

![](/static/images/from-lora-to-longlora/quantile.png)

Trong má»™t tensor, cÃ³ thá»ƒ xáº£y ra hiá»‡n tÆ°á»£ng outlier, tá»©c lÃ  cÃ³ má»™t sá»‘ giÃ¡ trá»‹ xuáº¥t hiá»‡n ráº¥t hiáº¿m, nhÆ°ng láº¡i cÃ³ giÃ¡ trá»‹ ráº¥t lá»›n hoáº·c ráº¥t nhá» so vá»›i cÃ¡c giÃ¡ trá»‹ cÃ²n láº¡i trong tensor. Nhá»¯ng giÃ¡ trá»‹ nÃ y thÆ°á»ng mang tÃ­nh quan trá»ng Ä‘áº·c biá»‡t vÃ  cáº§n pháº£i Ä‘Æ°á»£c biá»ƒu diá»…n chÃ­nh xÃ¡c. TÃ­nh cháº¥t cá»§a Quantile Quantize lÃ  cÃ¡c khoáº£ng quantile pháº£i cÃ³ xÃ¡c suáº¥t báº±ng nhau, dáº«n Ä‘áº¿n viá»‡c cÃ¡c giÃ¡ trá»‹ outlier nhÆ° 2.75 sáº½ Ä‘Æ°á»£c gá»™p vÃ o cÃ¡c khoáº£ng tá»« 1.5 trá»Ÿ Ä‘i. Äá»ƒ kháº¯c phá»¥c váº¥n Ä‘á» nÃ y, thay vÃ¬ quantize cáº£ tensor (bao gá»“m nhiá»u pháº§n tá»­) má»™t cÃ¡ch toÃ n diá»‡n, chÃºng ta sáº½ chia tensor thÃ nh nhiá»u pháº§n nhá» hÆ¡n (chunks) vÃ  quantize má»—i chunk cá»§a tensor Ä‘Ã³ riÃªng biá»‡t.

![](/static/images/from-lora-to-longlora/outlier.png)

QLoRA Ã¡p dá»¥ng ká»¹ thuáº­t Quantile Quantization trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. Tuy nhiÃªn, viá»‡c xÃ¡c Ä‘á»‹nh quantile phÃ¹ há»£p cho tá»«ng tensor weight Ä‘Ã²i há»i nhiá»u thá»i gian vÃ¬ tÃ­nh khÃ³ khÄƒn trong quÃ¡ trÃ¬nh nÃ y. Máº·c dÃ¹ cÃ³ cÃ¡c thuáº­t toÃ¡n xáº¥p xá»‰ quantile nhanh, nhÆ°ng chÃºng khÃ´ng pháº£n Ã¡nh tá»‘t sá»± xuáº¥t hiá»‡n cá»§a outlier. Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, QLoRA Ä‘Ã£ nháº­n ra ráº±ng náº¿u táº¥t cáº£ cÃ¡c tensor tuÃ¢n theo cÃ¹ng má»™t loáº¡i phÃ¢n phá»‘i cá»‘ Ä‘á»‹nh, chÃºng ta chá»‰ cáº§n xÃ¡c Ä‘á»‹nh cÃ¡c quantiles má»™t láº§n duy nháº¥t vÃ  Ã¡p dá»¥ng chÃºng cho toÃ n bá»™ cÃ¡c tensor weight thay vÃ¬ pháº£i tÃ¬m quantiles phÃ¹ há»£p cho tá»«ng tensor riÃªng láº». QLoRA Ä‘Ã£ tiáº¿n hÃ nh cÃ¡c thÃ­ nghiá»‡m Ä‘á»ƒ chá»©ng minh ráº±ng Ä‘a sá»‘ cÃ¡c tensor weights cá»§a pre-trained LLM tuÃ¢n theo phÃ¢n phá»‘i chuáº©n vá»›i mean 0 vÃ  standard deviation <span className="math-inline" data-math="\sigma"></span>. Do Ä‘Ã³, QLoRA Ä‘Ã£ Ä‘iá»u chá»‰nh toÃ n bá»™ cÃ¡c weights vá» má»™t phÃ¢n phá»‘i cá»‘ Ä‘á»‹nh báº±ng cÃ¡ch chia cho <span className="math-inline" data-math="\sigma"></span>, Ä‘áº£m báº£o ráº±ng phÃ¢n phá»‘i sáº½ náº±m trong khoáº£ng giÃ¡ trá»‹ cá»§a target dtype.

QLoRA muá»‘n quantize weights cá»§a máº¡ng nÆ¡-ron vá» má»™t khoáº£ng giÃ¡ trá»‹ cá»¥ thá»ƒ lÃ  <span className="math-inline" data-math="[-1, 1]"></span> cho target dtype. Tuy nhiÃªn, viá»‡c sá»­ dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p quantile thÃ´ng thÆ°á»ng sáº½ khÃ´ng thá»ƒ biá»ƒu diá»…n chÃ­nh xÃ¡c sá»‘ 0 (vá»‹ trÃ­ trung tÃ¢m, Q2). Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, sá»‘ quantiles <span className="math-inline" data-math="q_i"></span> Ä‘Æ°á»£c chia thÃ nh hai pháº§n: <span className="math-inline" data-math="2^{k-1}"></span> cho pháº§n Ã¢m kÃ¨m sá»‘ 0 vÃ  <span className="math-inline" data-math="2^{k-1} + 1"></span> cho pháº§n dÆ°Æ¡ng kÃ¨m sá»‘ 0. Sau Ä‘Ã³, hai pháº§n nÃ y Ä‘Æ°á»£c káº¿t há»£p láº¡i vÃ  má»™t sá»‘ 0 Ä‘Æ°á»£c loáº¡i bá» (do sá»‘ 0 Ä‘Æ°á»£c tÃ­nh hai láº§n). ÄÃ¢y lÃ  cÃ¡ch NF4 Ä‘Æ°á»£c táº¡o ra.

TÃºm cÃ¡i vÃ¡y láº¡i, NF4 cÃ³ cÃ¡c Ä‘áº·c Ä‘iá»ƒm sau:

* Sá»­ dá»¥ng 4 bits biá»ƒu diá»…n.
* Náº±m trong khoáº£ng <span className="math-inline" data-math="[âˆ’1,1]"></span>.
* Báº¥t Ä‘á»‘i xá»©ng, cÃ³ sá»± biá»ƒu diá»…n cho giÃ¡ trá»‹ 0.
* ÄÆ°á»£c táº¡o ra Ä‘á»ƒ Ã¡p dá»¥ng cho tensor tuÃ¢n theo phÃ¢n phá»‘i chuáº©n mean 0 vÃ  std 1.

Äá»ƒ thá»±c hiá»‡n quantize tensor weight vá» NF4, trÆ°á»›c tiÃªn ta pháº£i scale tensor weight vá» khoáº£ng <span className="math-inline" data-math="[âˆ’1,1]"></span>, sau Ä‘Ã³ ta thá»±c hiá»‡n quantize nhÆ° bÃ¬nh thÆ°á»ng. LÆ°u Ã½, bÆ°á»›c nÃ y sáº½ bao gá»“m Ã¡p dá»¥ng cáº£ Chunk Quantize.

### Double Quantization

Double Quantization, nhÆ° tÃªn gá»i cá»§a nÃ³, thá»±c hiá»‡n quÃ¡ trÃ¬nh quantize hai láº§n. Khi thá»±c hiá»‡n Chunk Quantize vÃ  sá»­ dá»¥ng nhiá»u chunks, má»—i chunk sáº½ cÃ³ má»™t háº±ng sá»‘ quantization riÃªng cá»§a nÃ³. Viá»‡c nÃ y dáº«n Ä‘áº¿n tÄƒng bá»™ nhá»› cáº§n thiáº¿t Ä‘á»ƒ lÆ°u trá»¯ cÃ¡c háº±ng sá»‘ quantization. Do Ä‘Ã³, QLoRA thá»±c hiá»‡n quÃ¡ trÃ¬nh quantize cho cáº£ cÃ¡c háº±ng sá»‘ quantization tá»« FP32 sang FP8.

### Paged Optimizers

Khi huáº¥n luyá»‡n mÃ´ hÃ¬nh trÃªn GPU, viá»‡c gáº·p lá»—i "Out of Memory" (OOM) lÃ  phá»• biáº¿n. Paged Optimizers giÃºp giáº£i quyáº¿t váº¥n Ä‘á» nÃ y báº±ng cÃ¡ch sá»­ dá»¥ng Unified Memory trÃªn GPU Nvidia. Khi xáº£y ra lá»—i OOM, cÃ¡c dá»¯ liá»‡u vÃ  tÃ i nguyÃªn khiáº¿n cho GPU bá»‹ OOM sáº½ Ä‘Æ°á»£c chuyá»ƒn táº¡m thá»i sang CPU, tá»©c lÃ  chuyá»ƒn dá»¯ liá»‡u tá»« bá»™ nhá»› VRAM sang RAM. Khi GPU cáº§n láº¡i cÃ¡c tÃ i nguyÃªn nÃ y Ä‘á»ƒ tÃ­nh toÃ¡n, chÃºng sáº½ Ä‘Æ°á»£c Ä‘Ã²i láº¡i tá»« CPU. Äiá»u nÃ y giÃºp trÃ¡nh lá»—i OOM vÃ  tiáº¿p tá»¥c quÃ¡ trÃ¬nh huáº¥n luyá»‡n mÃ´ hÃ¬nh má»™t cÃ¡ch hiá»‡u quáº£ trÃªn GPU.

## CÆ¡ cháº¿ hoáº¡t Ä‘á»™ng

QLoRA hiá»‡n táº¡i chá»‰ há»— trá»£ cho Linear layer (vÃ¬ LoRA cÅ©ng chá»‰ há»— trá»£ cho Linear). Má»™t layer sáº½ gá»“m 2 thÃ nh pháº§n: ThÃ nh pháº§n pretrained (freeze) vÃ  thÃ nh pháº§n LoRA (train). QuÃ¡ trÃ¬nh tÃ­nh toÃ¡n output cá»§a layer Linear cÃ³ LoRA Ä‘Ã³ nhÆ° sau:

<div className="math-block" data-math="Y^{BF16} = X^{BF16} \text{doubleDequant}(c_{1}^{FP32},c_{2}^{k-bit},W^{NF4}) + X^{BF16} L_1^{BF16} L_2^{BF16}"></div>

vá»›i <span className="math-inline" data-math="\text{doubleDequant}()"></span> nhÆ° sau:

<div className="math-block" data-math="\text{doubleDequant}(c_{1}^{FP32},c_{2}^{k-bit},W^{NF4}) = \text{dequant}(\text{dequant}(c_{1}^{FP32},c_{2}^{k-bit}),W^{NF4}) = W^{BF16}"></div>

Trong má»™t Linear layer cÃ³ LoRA, thÃ nh pháº§n Ä‘Æ°á»£c pretrained Ä‘Æ°á»£c Ä‘Ã³ng bÄƒng vÃ  khÃ´ng cáº­p nháº­t gradient, trong khi thÃ nh pháº§n LoRA Ä‘Æ°á»£c huáº¥n luyá»‡n. Trá»ng sá»‘ cá»§a thÃ nh pháº§n pretrained Ä‘Æ°á»£c quantize vá» NF4. Trong quÃ¡ trÃ¬nh tÃ­nh toÃ¡n output, thÃ nh pháº§n pretrained Ä‘Æ°á»£c dequantize tá»« NF4 vá» BF16, sau Ä‘Ã³ Ä‘Æ°á»£c káº¿t há»£p vá»›i thÃ nh pháº§n LoRA á»Ÿ dáº¡ng BF16. Khi tÃ­nh toÃ¡n hoÃ n táº¥t, thÃ nh pháº§n pretrained láº¡i Ä‘Æ°á»£c quantize vá» NF4. Nhá» váº­y, QLoRA giÃºp model vá»«a vá»›i VRAM Ä‘á»ƒ huáº¥n luyá»‡n, máº·c dÃ¹ khÃ´ng tÄƒng tá»‘c Ä‘á»™ cá»§a model (do cáº§n dequantize thÃ nh pháº§n cáº§n tÃ­nh toÃ¡n vá» BF16 táº¡i má»—i layer).

# LongLoRA lÃ  gÃ¬?

ChÃºng ta Ä‘Ã£ xong pháº§n nháº¹ hÆ¡n rá»“i, váº­y thÃ¬ dÃ i hÆ¡n á»Ÿ Ä‘Ã¢u mÃ  sÆ°á»›ng, vÃ  Ä‘Ã¢y, Ä‘Ãºng nhÆ° tÃªn gá»i cá»§a nÃ³, LongLoRA lÃ  kÄ© thuáº­t Ä‘á»ƒ kÃ©o dÃ i context vÄƒn báº£n Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ má»Ÿ rá»™ng kháº£ nÄƒng há»c cá»§a mÃ´ hÃ¬nh. Cháº³ng háº¡n, vá»›i Llama-2 7B, Ä‘á»™ dÃ i context lÃ  4096 tokens, Ä‘á»“ng nghÄ©a vá»›i viá»‡c nÃ³ chá»‰ cÃ³ thá»ƒ mÃ£ hÃ³a Ä‘Æ°á»£c Ä‘oáº¡n vÄƒn báº£n vá»›i 4096 tá»«. Äiá»u nÃ y gÃ¢y ra sá»± khÃ³ khÄƒn cho nhá»¯ng mÃ´ hÃ¬nh chuyÃªn dá»¥ng Ä‘á»ƒ truy xuáº¥t thÃ´ng tin tá»« sÃ¡ch, bÃ¡o. Do Ä‘Ã³, Yukang Chen cÃ¹ng Ä‘á»“ng bá»n Ä‘Ã£ nghiÃªn cá»©u vÃ  Ä‘á» xuáº¥t LongLoRA, vá»›i kháº£ nÄƒng má»Ÿ rá»™ng context lÃªn Ä‘áº¿n 100,000 tokens. Äiá»u nÃ y cÃ³ thá»ƒ cho phÃ©p báº¡n tÃ³m táº¯t ká»‹ch báº£n phim CÃ´ dÃ¢u 8 tuá»•i trong vÃ²ng 50 tá»« :D.

## CÆ¡ cháº¿ cá»§a LongLoRA

NÃ³i ngáº¯n gá»n ráº±ng, LongLoRA Ä‘áº¡t Ä‘Æ°á»£c chiá»u dÃ i context Ä‘Ã¡ng ká»ƒ báº±ng cÃ¡ch thay Ä‘á»•i hai yáº¿u tá»‘ sau:

* Shifted Sparse Attention (S2-Attn): NhÃ³m tÃ¡c giáº£ Ä‘á» xuáº¥t S2-Attn thay tháº¿ cho cÆ¡ cháº¿ multihead self-attention cÃ³ sáºµn trong Transformer Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™ vÃ  giáº£m chi phÃ­ tÃ­nh toÃ¡n.
* LongLoRA cho phÃ©p hai tham sá»‘ embedding vÃ  normalization thay Ä‘á»•i dá»±a trÃªn dá»¯ liá»‡u Ä‘Æ°a vÃ o (trainable). Äiá»u nÃ y cÃ³ vai trÃ² then chá»‘t Ä‘á»‘i vá»›i viá»‡c má»Ÿ rá»™ng context vÃ  chá»‰ giá»›i thiá»‡u má»™t sá»‘ lÆ°á»£ng tá»‘i thiá»ƒu cÃ¡c tham sá»‘ cÃ³ thá»ƒ training.

Pháº§n cÃ²n láº¡i, LongLoRA lÃ m hoÃ n toÃ n giá»‘ng há»‡t LoRA vÃ  khÃ´ng cÃ³ quÃ¡ nhiá»u sá»± khÃ¡c biá»‡t nÃ o Ä‘Ã¡ng ká»ƒ. Váº­y má»›i tháº¥y, viá»‡c Ä‘em 2 tham sá»‘ embedding vÃ  normalization ra Ä‘á»ƒ training cÃ³ sá»©c áº£nh hÆ°á»Ÿng lá»›n tá»›i performance cá»§a mÃ´ hÃ¬nh.

## Shifted Sparse Attention (S2-Attn)

![](/static/images/from-lora-to-longlora/s2attn.png)

NhÃ³m tÃ¡c giáº£ Ä‘á» xuáº¥t S2-Attn báº±ng cÃ¡ch dá»‹ch chuyá»ƒn cÃ¡c partition vá»›i khoáº£ng dá»‹ch chuyá»ƒn báº±ng má»™t ná»­a group size cá»§a attention head. VÃ­ dá»¥, xÃ©t context length 8192 á»Ÿ hÃ¬nh trÃªn, trong Pattern 1, nhÃ³m Ä‘áº§u tiÃªn tiáº¿n hÃ nh self attention tá»« token 1 Ä‘áº¿n 2048. Trong Pattern 2, group partition Ä‘Æ°á»£c dá»‹ch chuyá»ƒn 1024. NhÃ³m tiÃªn báº¯t Ä‘áº§u tá»« 1025 vÃ  káº¿t thÃºc á»Ÿ token thá»© 3072, trong khi 1024 token Ä‘áº§u tiÃªn vÃ  cuá»‘i cÃ¹ng thuá»™c cÃ¹ng má»™t nhÃ³m. S2-Attn Ä‘Æ°á»£c Ã¡p dá»¥ng báº±ng viá»‡c ghÃ©p cáº£ 2 pattern 1 vÃ  2 láº¡i vá»›i nhau, tÆ°Æ¡ng á»©ng á»Ÿ má»—i ná»­a self-attention head. CÃ¡ch nÃ y khÃ´ng lÃ m tÄƒng chi phÃ­ tÃ­nh toÃ¡n bá»• sung nhÆ°ng cho phÃ©p luá»“ng thÃ´ng tin giá»¯a cÃ¡c nhÃ³m khÃ¡c nhau.

Nghe thÃ¬ cÃ³ váº» phá»©c táº¡p Ä‘Ãºng khÃ´ng, nhÆ°ng thá»±c ra, S2-Attn hoÃ n toÃ n dá»… dÃ ng Ä‘á»ƒ Ã¡p dá»¥ng, vá»›i 2 bÆ°á»›c cÆ¡ báº£n:
* Dá»‹ch chuyá»ƒn token trong self-attention head,
* Chuyá»ƒn cÃ¡c feature tá»« token dimension sang batch dimension.

## Repository tham kháº£o
NhÃ³m tÃ¡c giáº£ Ä‘Ã£ cung cáº¥p repository cá»§a LongLoRA á»Ÿ [Ä‘Æ°á»ng link nÃ y](https://github.com/dvlab-research/LongLoRA). CÃ¡c báº¡n cÃ³ thá»ƒ tÃ¬m hiá»ƒu Ä‘á»c vÃ  thá»±c hiá»‡n vá»›i cÃ¡c mÃ´ hÃ¬nh cÃ³ sáºµn.


# Tham kháº£o

[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)

[LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/abs/2208.07339)

[LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models](https://arxiv.org/abs/2309.12307)

# ÄÃ´i lá»i cuá»‘i nÄƒm

HÃ´m nay cÅ©ng lÃ  29 Táº¿t rá»“i, cÃ³ láº½ má»i ngÆ°á»i cÅ©ng Ä‘Ã£ gÃ³i bÃ¡nh xong, Táº¥t niÃªn xong hoáº·c chuáº©n bá»‹ Táº¥t niÃªn, nhÆ°ng má»i ngÆ°á»i Ä‘Ã£ dÃ nh thá»i gian á»Ÿ Ä‘Ã¢y, Ä‘á»ƒ Ä‘á»c bÃ i viáº¿t nÃ y cho Ä‘áº¿n táº­n dÃ²ng cuá»‘i cÃ¹ng. Minh cáº£m Æ¡n táº¥t cáº£ má»i ngÆ°á»i Ä‘Ã£ quan tÃ¢m Ä‘áº¿n blog cá»§a mÃ¬nh vÃ  vÃ  chÃºc cho cÃ¡c báº¡n vÃ  gia Ä‘Ã¬nh má»™t nÄƒm má»›i háº¡nh phÃºc, bÃ¬nh an, vÃ  nhiá»u thÃ nh cÃ´ng hÆ¡n ná»¯a.

Happy new year!