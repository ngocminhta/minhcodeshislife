---
title: 'Transformer - KhÃ´ng pháº£i anh hÃ¹ng, Ä‘Ã¢y lÃ  ká»‰ nguyÃªn cá»§a GenAI'
date: '2024-01-08'
lastmod: '2023-01-08'
tags: ['deep learning', 'transformer']
draft: false
summary: 'Transformer - KhÃ´ng pháº£i anh hÃ¹ng, Ä‘Ã¢y lÃ  ká»‰ nguyÃªn cá»§a GenAI'
images: ['/static/images/fundamentals-of-transformer/transformer.jpg']
---

> ğŸ’¡ Trong blog nÃ y, mÃ¬nh sáº½ trÃ¬nh bÃ y chi tiáº¿t cÃ¡ch mÃ´ hÃ¬nh Transformer hoáº¡t Ä‘á»™ng, cÅ©ng nhÆ° lÃ  cÃ¡ch cÃ i Ä‘áº·t mÃ´ hÃ¬nh chi tiáº¿t cho nhá»¯ng báº¡n má»›i cÃ³ kiáº¿n thá»©c cÆ¡ báº£n vá» deep learning nhÆ° CNN hoáº·c LSTM cÅ©ng cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c.

Sá»± ná»•i tiáº¿ng cá»§a mÃ´ hÃ¬nh Transformer thÃ¬ khÃ´ng cáº§n pháº£i bÃ n cÃ£i, vÃ¬ nÃ³ chÃ­nh lÃ  ná»n táº£ng cá»§a ráº¥t nhiá»u mÃ´ hÃ¬nh khÃ¡c mÃ  ná»•i tiáº¿ng nháº¥t lÃ  BERT (Bidirectional Encoder Representations from Transformers) má»™t mÃ´ hÃ¬nh dÃ¹ng Ä‘á»ƒ há»c biá»ƒu diá»…n cá»§a cÃ¡c tá»« tá»‘t nháº¥t hiá»‡n táº¡i vÃ  Ä‘Ã£ táº¡o ra má»™t bÆ°á»›c ngoáº·t lá»›n cho Ä‘á»™ng Ä‘á»“ng NLP trong nÄƒm 2019. VÃ  chÃ­nh Google cÅ©ng Ä‘Ã£ Ã¡p dá»¥ng BERT trong cá»— mÃ¡y tÃ¬m kiáº¿m cá»§a há». Äá»ƒ hiá»ƒu BERT, cÃ¡c báº¡n cáº§n pháº£i náº¯m rÃµ vá» mÃ´ hÃ¬nh Transformer.

![](/static/images/fundamentals-of-transformer/bert.jpg)

Ã tÆ°á»Ÿng chá»§ Ä‘áº¡o cá»§a Transformer váº«n lÃ  Ã¡p dá»¥ng cÆ¡ thá»ƒ Attention, nhá»¯ng á»Ÿ má»©c phá»©c táº¡p hÆ¡n vÃ  tháº­t sá»± lÃ  thÃº vá»‹ hÆ¡n so vá»›i cÃ¡ch Ä‘Æ°á»£c Ä‘á» xuáº¥t trÆ°á»›c Ä‘Ã³ trong má»™t [bÃ i bÃ¡o](https://arxiv.org/abs/1508.04025) cá»§a anh LÆ°Æ¡ng Minh Tháº¯ng, má»™t ngÆ°á»i Viá»‡t ráº¥t ná»•i tiáº¿ng trong cá»™ng Ä‘á»“ng Deep Learning. 

# Tá»•ng Quan MÃ´ HÃ¬nh
Äá»ƒ cho dá»… cáº£m nháº­n Ä‘Æ°á»£c cÃ¡ch mÃ  mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng, mÃ¬nh sáº½ trÃ¬nh bÃ y trÆ°á»›c toÃ n bá»™ kiáº¿n trÃºc mÃ´ hÃ¬nh á»Ÿ má»©c high-level vÃ  sau Ä‘Ã³ sáº½ Ä‘i chi tiáº¿t tá»«ng pháº§n nhá» cÅ©ng nhÆ° cÃ´ng thá»©c toÃ¡n cá»§a nÃ³. 

Giá»‘ng nhÆ° nhá»¯ng mÃ´ hÃ¬nh dá»‹ch mÃ¡y khÃ¡c, kiáº¿n trÃºc tá»•ng quan cá»§a mÃ´ hÃ¬nh transformer bao gá»“m 2 pháº§n lá»›n lÃ  encoder vÃ  decoder. Encoder dÃ¹ng Ä‘á»ƒ há»c vector biá»ƒu cá»§a cÃ¢u vá»›i mong muá»‘n ráº±ng vector nÃ y mang thÃ´ng tin hoÃ n háº£o cá»§a cÃ¢u Ä‘Ã³. Decoder thá»±c hiá»‡n chá»©c nÄƒng chuyá»ƒn vector biá»ƒu diá»…n kia thÃ nh ngÃ´n ngá»¯ Ä‘Ã­ch.

Trong vÃ­ dá»¥ á»Ÿ dÆ°á»›i, encoder cá»§a mÃ´ hÃ¬nh transformer nháº­n má»™t cÃ¢u tiáº¿ng anh, vÃ  encode thÃ nh má»™t vector biá»ƒu diá»…n ngá»¯ nghÄ©a cá»§a cÃ¢u <i>little sun</i>, sau Ä‘Ã³ mÃ´ hÃ¬nh decoder nháº­n vector biá»ƒu diá»…n nÃ y, vÃ  dá»‹ch nÃ³ thÃ nh cÃ¢u tiáº¿ng viá»‡t <i>máº·t trá»i bÃ© nhá»</i>

![](/static/images/fundamentals-of-transformer/overview.jpg)

Má»™t trong nhá»¯ng Æ°u Ä‘iá»ƒm cá»§a transformer lÃ  mÃ´ hÃ¬nh nÃ y cÃ³ kháº£ nÄƒng xá»­ lÃ½ song song cho cÃ¡c tá»«. NhÆ° cÃ¡c báº¡n tháº¥y, Encoders cá»§a mÃ´ hÃ¬nh transformer lÃ  má»™t dáº¡ng feedforward neural nets, bao gá»“m nhiá»u encoder layer khÃ¡c, má»—i encoder layer nÃ y xá»­ lÃ½ Ä‘á»“ng thá»i cÃ¡c tá»«. Trong khi Ä‘Ã³, vá»›i mÃ´ hÃ¬nh LSTM, thÃ¬ cÃ¡c tá»« pháº£i Ä‘Æ°á»£c xá»­ lÃ½ tuáº§n tá»±. NgoÃ i ra, mÃ´ hÃ¬nh Transformer cÃ²n xá»­ lÃ½ cÃ¢u Ä‘áº§u vÃ o theo 2 hÆ°á»›ng mÃ  khÃ´ng cáº§n pháº£i stack thÃªm má»™t hÃ¬nh LSTM ná»¯a nhÆ° trong kiáº¿n trÃºc Bidirectional LSTM. 

![](/static/images/fundamentals-of-transformer/overview2.jpg)


Má»™t cÃ¡i nhÃ¬n vá»«a tá»•ng quÃ¡t vÃ  chi tiáº¿t sáº½ giÃºp Ã­ch cho cÃ¡c báº¡n. MÃ¬nh sáº½ Ä‘i vÃ o chi tiáº¿t má»™t sá»‘ pháº§n cá»±c kÃ¬ quan trá»ng nhÆ° sinusoidal position encoding, multi head attention cá»§a encoder, cÃ²n cá»§a decoder thÃ¬ cÃ¡c báº¡n tháº¥y Ä‘Æ°á»£c kiáº¿n trÃºc ráº¥t giá»‘ng vá»›i cá»§a encoder, do Ä‘Ã³ mÃ¬nh sáº½ chá»‰ Ä‘i nhanh qua mÃ  thÃ´i. 

![](/static/images/fundamentals-of-transformer/overview3.jpg)

# Embedding Layer with Position Encoding
TrÆ°á»›c khi Ä‘i vÃ o mÃ´ hÃ¬nh encoder, chÃºng ta sáº½ tÃ¬m hiá»ƒu cÆ¡ cháº¿ ráº¥t thÃº vá»‹ lÃ  Position Encoding dÃ¹ng Ä‘á»ƒ Ä‘Æ°a thÃ´ng tin vá» vá»‹ trÃ­ cá»§a cÃ¡c tá»« vÃ o mÃ´ hÃ¬nh transformer. 

Äáº§u tiÃªn, cÃ¡c tá»« Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng má»™t vector sá»­ dá»¥ng má»™t ma tráº­n word embedding cÃ³ sá»‘ dÃ²ng báº±ng kÃ­ch thÆ°á»›c cá»§a táº­p tá»« vá»±ng. Sau Ä‘Ã³ cÃ¡c tá»« trong cÃ¢u Ä‘Æ°á»£c tÃ¬m kiáº¿m trong ma tráº­n nÃ y, vÃ  Ä‘Æ°á»£c ná»‘i nhau thÃ nh cÃ¡c dÃ²ng cá»§a má»™t ma tráº­n 2 chiá»u chá»©a ngá»¯ nghÄ©a cá»§a tá»«ng tá»« riÃªng biá»‡t. NhÆ°ng nhÆ° cÃ¡c báº¡n Ä‘Ã£ tháº¥y, transformer xá»­ lÃ½ cÃ¡c tá»« song song, do Ä‘Ã³, vá»›i chá»‰ word embedding mÃ´ hÃ¬nh khÃ´ng thá»ƒ nÃ o biáº¿t Ä‘Æ°á»£c vá»‹ trÃ­ cÃ¡c tá»«. NhÆ° váº­y, chÃºng ta cáº§n má»™t cÆ¡ cháº¿ nÃ o Ä‘Ã³ Ä‘á»ƒ Ä‘Æ°a thÃ´ng tin vá»‹ trÃ­ cÃ¡c tá»« vÃ o trong vector Ä‘áº§u vÃ o. ÄÃ³ lÃ  lÃºc positional encoding xuáº¥t hiá»‡n vÃ  giáº£i quyáº¿t váº¥n Ä‘á» cá»§a chÃºng ta. Tuy nhiÃªn, trÆ°á»›c khi giá»›i thiá»‡u cÆ¡ cháº¿ position encoding cá»§a tÃ¡c giáº£, cÃ¡c báº¡n cÃ³ thá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» bÄƒng má»™t sá»‘ cÃ¡ch naive nhÆ° sau:

Biá»ƒu diá»…n vá»‹ trÃ­ cÃ¡c tá»« báº±ng chuá»—i cÃ¡c sá»‘ liÃªn tá»¥c tá»« 0,1,2,3 ..., n. Tuy nhiÃªn, chÃºng ta gáº·p ngay váº¥n Ä‘á» lÃ  khi chuá»—i dÃ i thÃ¬ sá»‘ nÃ y cÃ³ thá»ƒ khÃ¡ lá»›n, vÃ  mÃ´ hÃ¬nh sáº½ gáº·p khÃ³ khÄƒn khi dá»± Ä‘oÃ¡n nhá»¯ng cÃ¢u cÃ³ chiá»u dÃ i lá»›n hÆ¡n táº¥t cáº£ cÃ¡c cÃ¢u cÃ³ trong táº­p huáº¥n luyá»‡n. Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, cÃ¡c báº¡n cÃ³ thá»ƒ chuáº©n hÃ³a láº¡i cho chuá»—i sá»‘ nÃ y náº±m trong Ä‘oáº¡n tá»« 0-1 báº±ng cÃ¡ch chia cho n nhÆ°ng mÃ  chÃºng ta sáº½ gáº·p váº¥n Ä‘á» khÃ¡c lÃ  khoáº£ng cÃ¡ch giá»¯a 2 tá»« liÃªn tiáº¿p sáº½ phá»¥ thuá»™c vÃ o chiá»u dÃ i cá»§a chuá»—i, vÃ  trong má»™t khoáº£n cá»‘ Ä‘á»‹nh, chÃºng ta khÃ´ng hÃ¬nh dÃ¹ng Ä‘Æ°á»£c khoáº£n Ä‘Ã³ chá»©a bao nhiÃªu tá»«. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  Ã½ nghÄ©a cá»§a position encoding sáº½ khÃ¡c nhau tÃ¹y thuá»™c vÃ o Ä‘á»™ dÃ i cá»§a cÃ¢u Ä‘Ã³.

## PhÆ°Æ¡ng phÃ¡p Ä‘á» xuáº¥t sinusoidal position encoding
PhÆ°Æ¡ng phÃ¡p cá»§a tÃ¡c giáº£ Ä‘á» xuáº¥t khÃ´ng gáº·p nhá»¯ng háº¡n cháº¿ mÃ  chÃºng ta vá»«a nÃªu. Vá»‹ trÃ­ cá»§a cÃ¡c tá»« Ä‘Æ°á»£c mÃ£ hÃ³a báº±ng má»™t vector cÃ³ kÃ­ch thÆ°á»›c báº±ng word embedding vÃ  Ä‘Æ°á»£c cá»™ng trá»±c tiáº¿p vÃ o word embedding. 

![](/static/images/fundamentals-of-transformer/embedding.jpg)

Cá»¥ thá»ƒ, táº¡i vá»‹ trÃ­ cháºµn, tÃ¡c giáº£ sá»­ dá»¥ng hÃ m sin, vÃ  vá»›i vá»‹ trÃ­ láº½ tÃ¡c giáº£ sá»­ dá»¥ng hÃ m cos Ä‘á»ƒ tÃ­nh giÃ¡ trá»‹ táº¡i chiá»u Ä‘Ã³.

<div className="math-block" data-math="p_t^i = f(t)^i = \sin(w_k \cdot t)"></div>

Trong Ä‘Ã³,

<div className="math-block" data-math="w_{k} = \dfrac{1}{10000^{2k/d}}"></div>

Trong hÃ¬nh dÆ°á»›i nÃ y, mÃ¬nh minh há»a cho cÃ¡ch tÃ­nh position encoding cá»§a tÃ¡c giáº£. Giáº£ sá»­ chÃºng ta cÃ³ word embedding cÃ³ 6 chiá»u, thÃ¬ position encoding cÅ©ng cÃ³ tÆ°Æ¡ng á»©ng lÃ  6 chiá»u. Má»—i dÃ²ng tÆ°Æ¡ng á»©ng vá»›i má»™t tá»«. GiÃ¡ trá»‹ cá»§a cÃ¡c vector táº¡i má»—i vá»‹ trÃ­ Ä‘Æ°á»£c tÃ­nh toÃ¡n theo cÃ´ng thá»©c á»Ÿ hÃ¬nh dÆ°á»›i. 

![](/static/images/fundamentals-of-transformer/pe.png)

LÃºc nÃ y má»™t sá»‘ báº¡n sáº½ tháº¯c máº¯c táº¡i sao vá»›i cÃ¡ch biá»ƒu diá»…n vá»‹ trÃ­ nhÆ° tÃ¡c giáº£ Ä‘á» xuáº¥t láº¡i cÃ³ thá»ƒ mÃ£ hÃ³a thÃ´ng tin vá»‹ trÃ­ cá»§a tá»«? HÃ£y tÆ°á»Ÿng tÆ°á»£ng báº¡n cÃ³ cÃ¡c sá»‘ tá»« 0-15. CÃ¡c báº¡n cÃ³ thá»ƒ tháº¥y ráº±ng bit ngoÃ i cÃ¹ng bÃªn pháº£i thay Ä‘á»•i nhanh nháº¥t má»—i 1 sá»‘, vÃ  sau Ä‘Ã³ lÃ  bit bÃªn pháº£i thá»© 2, thay Ä‘á»•i má»—i 2 sá»‘, tÆ°Æ¡ng tá»± cho cÃ¡c bit khÃ¡c. 

![](/static/images/fundamentals-of-transformer/pe_intuition.jpg)

Trong cÃ´ng thá»©c cá»§a tÃ¡c giáº£ Ä‘á» xuáº¥t, cÃ¡c báº¡n cÅ©ng tháº¥y ráº±ng, hÃ m sin vÃ  cos cÃ³ dáº¡ng Ä‘á»“ thá»‹ táº§n sá»‘ vÃ  táº§n sá»‘ nÃ y giáº£m dáº§n á»Ÿ cÃ¡c chiá»u lá»›n dáº§n. CÃ¡c báº¡n xem hÃ¬nh dÆ°á»›i, á»Ÿ chiá»u 0, giÃ¡ trá»‹ thay Ä‘á»•i liÃªn tá»¥c tÆ°Æ¡ng á»©ng vá»›i mÃ u sáº¯c thay Ä‘á»•i liÃªn tá»¥c, vÃ  táº§n sá»‘ thay Ä‘á»•i nÃ y giáº£m dáº§n á»Ÿ cÃ¡c chiá»u lá»›n hÆ¡n. 

![](/static/images/fundamentals-of-transformer/pe_heatmap.png)

NÃªn chÃºng ta cÃ³ thá»ƒ cáº£m nháº­n Ä‘Æ°á»£c viá»‡c biá»ƒu diá»…n cá»§a tÃ¡c giáº£ khÃ¡ tÆ°Æ¡ng tá»± nhÆ° cÃ¡ch biá»ƒu diá»…n cÃ¡c sá»‘ nguyÃªn trong há»‡ nhá»‹ phÃ¢n, cho nÃªn chÃºng ta cÃ³ thá»ƒ biá»ƒu diá»…n Ä‘Æ°á»£c vá»‹ trÃ­ cÃ¡c tá»« theo cÃ¡ch nhÆ° váº­y.

ChÃºng ta cÅ©ng cÃ³ thá»ƒ xem ma tráº­n khoáº£ng cÃ¡ch cá»§a cÃ¡c vector biá»ƒu diá»…n vá»‹ trÃ­ nhÆ° hÃ¬nh dÆ°á»›i. RÃµ rÃ ng, cÃ¡c vector biá»ƒu diá»…n thá»ƒ hiá»‡n Ä‘Æ°á»£c tÃ­nh cháº¥t khoáº£ng cÃ¡ch giá»¯a 2 tá»«. 2 tá»« cÃ¡ch cÃ ng xa nhau thÃ¬ khoáº£ng cÃ¡ch cÃ ng lá»›n hÆ¡n.

![](/static/images/fundamentals-of-transformer/pe_distance.png)

NgoÃ i ra, má»™t tÃ­nh cháº¥t cá»§a phÆ°Æ¡ng phÃ¡p tÃ¡c giáº£ Ä‘á» xuáº¥t lÃ  nÃ³ cho phÃ©p mÃ´ hÃ¬nh dá»… dÃ ng há»c Ä‘Æ°á»£c má»‘i quan há»‡ tÆ°Æ¡ng Ä‘á»‘i giá»¯a cÃ¡c tá»«. Cá»¥ thá»ƒ, biá»ƒu diá»…n vá»‹ trÃ­ cá»§a tá»« t + offset cÃ³ thá»ƒ chuyá»ƒn thÃ nh biá»ƒu diá»…n vá»‹ trÃ­ cá»§a tá»« t báº±ng má»™t phÃ©p biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh dá»±a trÃªn ma tráº­n phÃ©p quay. 

Äá»ƒ dá»… hÃ¬nh dung phÆ°Æ¡ng phÃ¡p cá»§a tÃ¡c giáº£ Ä‘á» xuáº¥t láº¡i hoáº¡t Ä‘á»™ng tá»‘t, cÃ¡c báº¡n cÃ³ thá»ƒ tÆ°á»Ÿng tÆ°á»£ng, hÃ m sin, vÃ  cos, giá»‘ng nhÆ° lÃ  kim giÃ¢y vÃ  kim phÃºt trÃªn Ä‘á»“ng há»“. Vá»›i 2 kim nÃ y, chÃºng ta cÃ³ thá»ƒ biá»ƒu diá»…n Ä‘Æ°á»£c 3600 vá»‹ trÃ­. VÃ  Ä‘á»“ng thá»i cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c ngay táº¡i sao biá»ƒu diá»…n cá»§a tá»« t + offset vÃ  tá»« t láº¡i cÃ³ thá»ƒ dá»… dÃ ng chuyá»ƒn Ä‘á»•i cho nhau.

# Encoder
Encoder cá»§a mÃ´ hÃ¬nh transformer cÃ³ thá»ƒ bao gá»“m nhiá»u encoder layer tÆ°á»£ng tá»± nhau. Má»—i encoder layer cá»§a transformer láº¡i bao gá»“m 2 thÃ nh pháº§n chÃ­nh lÃ  multi head attention vÃ  feedforward network, ngoÃ i ra cÃ²n cÃ³ cáº£ skip connection vÃ  normalization layer. 

Trong 2 thÃ nh pháº§n chÃ­nh nÃ y, cÃ¡c báº¡n sáº½ há»©ng thÃº nhiá»u hÆ¡n vá» multi-head attention vÃ¬ Ä‘Ã³ lÃ  má»™t layer má»›i Ä‘Æ°á»£c giá»›i thiá»‡u trong bÃ i bÃ¡o nÃ y, vÃ  chÃ­nh nÃ³ táº¡o nÃªn sá»± khÃ¡c biá»‡t giá»¯a mÃ´ hÃ¬nh LSTM vÃ  mÃ´ hÃ¬nh Transformer mÃ  chÃºng ta Ä‘ang tÃ¬m hiá»ƒu. 

![](/static/images/fundamentals-of-transformer/encoder.jpg)

Encoder Ä‘áº§u tiÃªn sáº½ nháº­n ma tráº­n biá»ƒu diá»…n cá»§a cÃ¡c tá»« Ä‘Ã£ Ä‘Æ°á»£c cá»™ng vá»›i thÃ´ng tin vá»‹ trÃ­ thÃ´ng qua positional encoding. Sau Ä‘Ã³, ma tráº­n nÃ y sáº½ Ä‘Æ°á»£c xá»­ lÃ½ bá»Ÿi Multi Head Attention. Multi Head Attention tháº­t cháº¥t lÃ  self-attention, nhÆ°ng mÃ  Ä‘á»ƒ mÃ´ hÃ¬nh cÃ³ thá»ƒ cÃ³ chÃº Ã½ nhiá»u pattern khÃ¡c nhau, tÃ¡c giáº£ Ä‘Æ¡n giáº£n lÃ  sá»­ dá»¥ng nhiá»u self-attention.

## Self Attention Layer
Self Attention cho phÃ©p mÃ´ hÃ¬nh khi mÃ£ hÃ³a má»™t tá»« cÃ³ thá»ƒ sá»­ dá»¥ng thÃ´ng tin cá»§a nhá»¯ng tá»« liÃªn quan tá»›i nÃ³. VÃ­ dá»¥ khi tá»« **nÃ³** Ä‘Æ°á»£c mÃ£ hÃ³a, nÃ³ sáº½ chÃº Ã½ vÃ o cÃ¡c tá»« liÃªn quan nhÆ° lÃ  **máº·t trá»i**.

![](/static/images/fundamentals-of-transformer/self_attention.jpg)

Báº¡n cÃ³ thá»ƒ tÆ°á»Ÿng tÆ°á»£ng cÆ¡ cháº¿ self attention giá»‘ng nhÆ° cÆ¡ cháº¿ tÃ¬m kiáº¿m. Vá»›i má»™t tá»« cho trÆ°á»›c, cÆ¡ cháº¿ nÃ y sáº½ cho phÃ©p mÃ´ hÃ¬nh tÃ¬m kiáº¿m trong cÃ¡ch tá»« cÃ²n láº¡i, tá»« nÃ o "giá»‘ng" Ä‘á»ƒ  sau Ä‘Ã³ thÃ´ng tin sáº½ Ä‘Æ°á»£c mÃ£ hÃ³a dá»±a trÃªn táº¥t cáº£ cÃ¡c tá»« trÃªn. 

Äáº§u tiÃªn, vá»›i mÃ´i tá»« chÃºng ta cáº§n táº¡o ra 3 vector: query, key, value vector báº±ng cÃ¡ch nhÃ¢n ma tráº­n biá»ƒu diá»…n cÃ¡c tá»« Ä‘áº§u vÃ o vá»›i ma tráº­n há»c tÆ°Æ¡ng á»©ng.
* query vector: vector dÃ¹ng Ä‘á»ƒ chá»©a thÃ´ng tin cá»§a tá»« Ä‘Æ°á»£c tÃ¬m kiáº¿m, so sÃ¡nh. Giá»‘ng nhÆ° lÃ  cÃ¢u query cá»§a google search. 
* key vector: vector dÃ¹ng Ä‘á»ƒ biá»ƒu diá»…n thÃ´ng tin cÃ¡c tá»« Ä‘Æ°á»£c so sÃ¡nh vá»›i tá»« cáº§n tÃ¬m kiáº¿m á»Ÿ trÃªn. VÃ­ dá»¥, nhÆ° cÃ¡c trang web mÃ  google sáº½ so sÃ¡nh vá»›i tá»« khÃ³a mÃ  báº¡n tÃ¬m kiáº¿m. 
* value vector: vector biá»ƒu diá»…n ná»™i dung, Ã½ nghÄ©a cá»§a cÃ¡c tá»«. CÃ¡c báº¡n cÃ³ thá»ƒ tÆ°á»£ng tÆ°á»£ng, nÃ³ nhÆ° lÃ  ná»™i dung trang web Ä‘Æ°á»£c hiá»ƒn thá»‹ cho ngÆ°á»i dÃ¹ng sau khi tÃ¬m kiáº¿m.

Äá»ƒ tÃ­nh tÆ°Æ¡ng quan, chÃºng ta Ä‘Æ¡n giáº£n chá»‰ cáº§n tÃ­nh tÃ­ch vÃ´ hÆ°á»›ng dá»±a cÃ¡c vector query vÃ  key. Sau Ä‘Ã³ dÃ¹ng hÃ m softmax Ä‘á»ƒ chuáº©n hÃ³a chá»‰ sá»‘ tÆ°Æ¡ng quan trong Ä‘oáº¡n 0-1, vÃ  cuá»‘i cÃ¹ng, tÃ­nh trung bÃ¬nh cá»™ng cÃ³ trá»ng sá»‘ giá»¯a cÃ¡c vector values sá»­ dá»¥ng chá»‰ sá»‘ tÆ°Æ¡ng quan má»›i tÃ­nh Ä‘Æ°á»£c. QuÃ¡ dá»… !!! 

![](/static/images/fundamentals-of-transformer/self_attention_2.png)

Cá»¥ thá»ƒ hÆ¡n, quÃ¡ trÃ¬nh tÃ­nh toÃ¡n attention vector cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ³m táº¯t lÃ m 3 bÆ°á»›c nhÆ° sau:
* BÆ°á»›c 1: TÃ­nh ma tráº­n query, key, value báº±ng cÃ¡ch khá»Ÿi táº¡o 3 ma tráº­n trá»ng sá»‘ query, key, vector. Sau Ä‘Ã³ nhÃ¢n input vá»›i cÃ¡c ma tráº­n trá»ng sá»‘ nÃ y Ä‘á»ƒ táº¡o thÃ nh 3 ma tráº­n tÆ°Æ¡ng á»©ng. 
* BÆ°á»›c 2: TÃ­nh attention weights. NhÃ¢n 2 ma tráº­n key, query vá»«a Ä‘Æ°á»£c tÃ­nh á»Ÿ trÃªn vá»›i nhau Ä‘á»ƒ vá»›i Ã½ nghÄ©a lÃ  so sÃ¡nh giá»¯a cÃ¢u query vÃ  key Ä‘á»ƒ há»c má»‘i tÆ°Æ¡ng quan. Sau Ä‘Ã³ thÃ¬ chuáº©n hÃ³a vá» Ä‘oáº¡n [0-1] báº±ng hÃ m softmax. 1 cÃ³ nghÄ©a lÃ  cÃ¢u query giá»‘ng vá»›i key, 0 cÃ³ nghÄ©a lÃ  khÃ´ng giá»‘ng.
* BÆ°á»›c 3: TÃ­nh output. NhÃ¢n attention weights vá»›i ma tráº­n value. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  chÃºng ta biá»ƒu diá»…n má»™t tá»« báº±ng trung bÃ¬nh cÃ³ trá»ng sá»‘ (attention weights) cá»§a ma tráº­n  value.

![](/static/images/fundamentals-of-transformer/attention_vector.jpg)


## Multi Head Attention
ChÃºng ta muá»‘n mÃ´ hÃ¬nh cÃ³ thá»ƒ  há»c nhiá»u kiá»ƒu má»‘i quan há»‡ giá»¯a cÃ¡c tá»« vá»›i nhau. Vá»›i má»—i self-attention, chÃºng ta há»c Ä‘Æ°á»£c má»™t kiá»ƒu pattern, do Ä‘Ã³ Ä‘á»ƒ cÃ³ thá»ƒ má»Ÿ rá»™ng kháº£ nÄƒng nÃ y, chÃºng ta Ä‘Æ¡n giáº£n lÃ  thÃªm nhiá»u self-attention. Tá»©c lÃ  chÃºng ta cáº§n nhiá»u ma tráº­n query, key, value mÃ  thÃ´i. Giá» Ä‘Ã¢y ma tráº­n trá»ng sá»‘ key, query, value sáº½ cÃ³ thÃªm 1 chiá»u depth ná»¯a. 

![](/static/images/fundamentals-of-transformer/multi_head_attention.jpg)

Multi head attention cho phÃ©p mÃ´ hÃ¬nh chÃº Ã½ Ä‘áº¿n Ä‘á»“ng thá»i nhá»¯ng pattern dá»… quan sÃ¡t Ä‘Æ°á»£c nhÆ° sau. 
* ChÃº Ã½ Ä‘áº¿n tá»« káº¿ trÆ°á»›c cá»§a má»™t tá»« 
* ChÃº Ã½ Ä‘áº¿n tá»« káº¿ sau cá»§a má»™t tá»«
* ChÃº Ã½ Ä‘áº¿n nhá»¯ng tá»« liÃªn quan cá»§a má»™t tá»«

## Residuals Connection vÃ  Normalization Layer
Trong kiáº¿n trÃºc cá»§a mÃ´ hÃ¬nh transformer, residuals connection vÃ  normalization layer Ä‘Æ°á»£c sá»­ dá»¥ng má»i nÆ¡i, giá»‘ng nhÆ° tinh tháº§n cá»§a nÃ³. 2 ká»¹ thuáº­t giÃºp cho mÃ´ hÃ¬nh huáº¥n luyá»‡n nhanh há»™i tá»¥ hÆ¡n vÃ  trÃ¡ch máº¥t mÃ¡t thÃ´ng tin trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n mÃ´ hÃ¬nh, vÃ­ dá»¥ nhÆ° lÃ  thÃ´ng tin cá»§a vá»‹ trÃ­ cÃ¡c tá»« Ä‘Æ°á»£c mÃ£ hÃ³a. 

# Decoder
Decoder thá»±c hiá»‡n chá»©c nÄƒng giáº£i mÃ£ vector cá»§a cÃ¢u nguá»“n thÃ nh cÃ¢u Ä‘Ã­ch, do Ä‘Ã³ decoder sáº½ nháº­n thÃ´ng tin tá»« encoder lÃ  2 vector key vÃ  value. Kiáº¿n trÃºc cá»§a decoder ráº¥t giá»‘ng vá»›i encoder, ngoáº¡i trá»« cÃ³ thÃªm má»™t multi head attention náº±m á»Ÿ giá»¯a dÃ¹ng Ä‘á»ƒ há»c má»‘i liÃªn quan giá»¯ tá»« Ä‘ang Ä‘Æ°á»£c dá»‹ch vá»›i cÃ¡c tá»« Ä‘Æ°á»£c á»Ÿ cÃ¢u nguá»“n. 

![](/static/images/fundamentals-of-transformer/decoder.jpg)

## Masked Multi Head Attention
Masked Multi Head Attention táº¥t nhiÃªn lÃ  multi head attention mÃ  chÃºng ta Ä‘Ã£ nÃ³i Ä‘áº¿n á»Ÿ trÃªn, cÃ³ chá»©c nÄƒng dÃ¹ng Ä‘á»ƒ encode cÃ¡c tá»« cÃ¢u cÃ¢u Ä‘Ã­ch trong quÃ¡ trÃ¬nh dá»‹ch, tuy nhiÃªn, lÃºc cÃ i Ä‘áº·t chÃºng ta cáº§n lÆ°u Ã½ ráº±ng pháº£i che Ä‘i cÃ¡c tá»« á»Ÿ tÆ°Æ¡ng lai chÆ°a Ä‘Æ°á»£c mÃ´ hÃ¬nh dá»‹ch Ä‘áº¿n, Ä‘á»ƒ lÃ m viá»‡c nÃ y thÃ¬ Ä‘Æ¡n giáº£n lÃ  chÃºng ta chá»‰ cáº§n nhÃ¢n vá»›i má»™t vector chá»©a cÃ¡c giÃ¡ trá»‹ 0,1. 

Trong decoder cÃ²n cÃ³ má»™t multi head attention khÃ¡c cÃ³ chá»©c nÄƒng chÃº Ã½ cÃ¡c tá»« á»Ÿ mÃ´ hÃ¬nh encoder, layer nÃ y nháº­n vector key vÃ  value tá»« mÃ´ hÃ¬nh encoder, vÃ  output tá»« layer phÃ­a dÆ°á»›i. ÄÆ¡n giáº£n bá»Ÿi vÃ¬ chÃºng ta muá»‘n so sÃ¡nh sá»± tÆ°Æ¡ng quan giá»¯a tá»« Ä‘ang Ä‘Æ°á»£c dá»‹ch vá»›i cÃ¡c tá»« nguá»“n. 

## Final Fully Connected Layer, Softmax vÃ  Loss function
Giá»‘ng nhÆ° nhiá»u mÃ´ hÃ¬nh khÃ¡c, chÃºng ta cáº§n thÃªm má»™t fully connected layer Ä‘á»ƒ chuyá»ƒn output tá»« layer phÃ­a trÆ°á»›c thÃ nh ma tráº­n cÃ³ chiá»u bÄƒng sá»‘ tá»« mÃ  cÃ¡c báº¡n cáº§n dá»± Ä‘oÃ¡n. Sau Ä‘Ã³ thÃ¬ Ä‘áº¿n softmax Ä‘á»ƒ cÃ¡c báº¡n tÃ­nh Ä‘Æ°á»£c xÃ¡c suáº¥t cá»§a tá»« xuáº¥t hiá»‡n tiáº¿p theo lÃ  bao nhiÃªu. 

Loss function thÃ¬ táº¥t nhiÃªn lÃ  cross-entropy mÃ  thÃ´i, giá»‘ng nhÆ° á»Ÿ cÃ¡c mÃ´ hÃ¬nh phÃ¢n loáº¡i khÃ¡c mÃ  cÃ¡c báº¡n Ä‘Ã£ lÃ m quen. 

## CÃ¡c ká»¹ thuáº­t Ä‘áº·c biá»‡t Ä‘á»ƒ huáº¥n luyá»‡n Transformer 
Äá»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh transformer, cÃ¡c báº¡n cáº§n pháº£i biáº¿t Ä‘áº¿n 2 ká»¹ thuáº­t ráº¥t thÃº vá»‹ nÃ y. Náº¿u khÃ´ng sá»­ dá»¥ng ká»¹ thuáº­t Ä‘áº§u tiÃªn vá» optimizer thÃ¬ mÃ´ hÃ¬nh transformer sáº½ **khÃ´ng há»™i tá»¥** Ä‘Æ°á»£c luÃ´n Ä‘áº¥y :))

### Optimizer
Äá»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh transformer, cÃ¡c báº¡n váº«n sá»­ dá»¥ng Adam, tuy nhiÃªn, learning rate cáº§n pháº£i Ä‘Æ°á»£c Ä‘iá»u chá»‰nh trong suá»‘t quÃ¡ trÃ¬nh há»c theo cÃ´ng thá»©c sau

<div className="math-block" data-math="\text{lr\_rate} = d_{\text{model}}^{-0.5} \cdot \min(\text{step\_num}^{-0.5}, \text{step\_num} \cdot \text{warmup\_steps}^{-1.5})"></div>

![](/static/images/fundamentals-of-transformer/opt.png)

CÆ¡ báº£n thÃ¬ learning rate sáº½ tÄƒng dáº§n trong cÃ¡c láº§n cáº­p nháº­t Ä‘áº§u tiÃªn, cÃ¡c bÆ°á»›c nÃ y Ä‘Æ°á»£c gá»i lÃ  warm up step, lÃºc nÃ y mÃ´ hÃ¬nh  sáº½ 'cháº¡y' táº¹t ga. Sau Ä‘Ã³ learning rate láº¡i giáº£m dáº§n, Ä‘á»ƒ mÃ´ hÃ¬nh há»™i tá»¥. 

### Label Smoothing
Vá»›i mÃ´ hÃ¬nh nhiá»u triá»‡u tham sá»‘ cá»§a transformer, thÃ¬ viá»‡t overfit lÃ  chuyá»‡n dá»… dÃ ng xáº£y ra. Äá»ƒ háº¡n cháº¿ hiá»‡n tÆ°á»£ng overfit, cÃ¡c báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng ká»¹ thuáº­t label smoothing. Vá» cÆ¡ báº£n thÃ¬ Ã½ tÆ°á»Ÿng cá»§a ká»¹ thuáº­t nÃ y khÃ¡ Ä‘Æ¡n giáº£n, chÃºng ta sáº½ pháº¡t mÃ´ hÃ¬nh khi nÃ³ quÃ¡ tá»± tin vÃ o viá»‡c dá»± Ä‘oÃ¡n cá»§a mÃ¬nh. Thay vÃ¬ mÃ£ hÃ³a nhÃ£n lÃ  má»™t one-hot vector, cÃ¡c báº¡n sáº½ thay Ä‘á»•i nhÃ£n nÃ y má»™t chÃºt báº±ng cÃ¡ch phÃ¢n bá»‘ má»™t tÃ­ xÃ¡c suáº¥t vÃ o cÃ¡c trÆ°á»ng há»£p cÃ²n láº¡i. 

![](/static/images/fundamentals-of-transformer/label_smoothing.jpg)

Giá» thÃ¬ cÃ¡c báº¡n sáº½ an tÃ¢m khi cÃ³ thá»ƒ Ä‘á»ƒ sá»‘ epoch lá»›n mÃ  khÃ´ng lo ráº±ng mÃ´ hÃ¬nh sáº½ overfit náº·ng ná». 

# Visualization
Visualize trong sá»‘ cá»§a cÃ¡c mÃ´ hÃ¬nh sá»­ dá»¥ng cÆ¡ cháº¿ attention thá»±c sá»± ráº¥t thÃº vá»‹. Trong mÃ´ hÃ¬nh transformer, chÃºng ta visualize táº¡i encoder vÃ  táº¡i decoder. 
CÃ¡c báº¡n cÃ³ thá»ƒ visualize Ä‘á»“ng thá»i táº¡i cÃ¡c heads cá»§a multi-head attentions, vÃ  táº¡i layers khÃ¡c nhau. 

## Encoder Visualize
CÃ¡c báº¡n cÃ³ thá»ƒ dÃ¹ng heatmap Ä‘á»ƒ visualize giÃ¡ trá»‹ attention, sáº½ cho chÃºng ta biáº¿t khi encode má»™t cÃ¢u mÃ´ hÃ¬nh chÃº Ã½ tá»« gÃ¬ á»Ÿ lÃ¢n cáº­n

![](/static/images/fundamentals-of-transformer/encoder_visualize.png)

á» Ä‘Ã¢y mÃ¬nh visualize giÃ¡ trá»‹ attention cá»§a encoder layer sá»‘ 2 vÃ  4, táº¡i cÃ¡c head 0,1,2,3 (trong cÃ i Ä‘áº·t cÃ¡c báº¡n cÃ³ tá»•ng cá»™ng 6 encoder layer vÃ  8 heads nhÃ©). NhÃ¬n vÃ o cÃ¡c heatmaps á»Ÿ trÃªn, cÃ¡c báº¡n cÃ³ thá»ƒ tháº¥y Ä‘Æ°á»£c ráº±ng khi encode má»™t tá»« mÃ´ hÃ¬nh sáº½ nhÃ¬n vÃ o cÃ¡c tá»« liÃªn quan xung quanh. VÃ­ dá»¥ tá»« **family** cÃ³ thá»ƒ  Ä‘Æ°á»£c mÃ£ hÃ³a báº±ng 2 tá»« liÃªn quan nhÆ° **my** vÃ  **family**. 

## Decoder Visualize
á» decoder, cÃ¡c báº¡n cÃ³ 2 loáº¡i visualization
* self attention: giÃ¡ trá»‹ attention khi mÃ´ hÃ¬nh decoder mÃ£ hÃ³a cÃ¢u Ä‘Ã­ch lÃºc dá»‹ch
* src attention: giÃ¡ trá»‹ attention khi mÃ´ hÃ¬nh decoder sá»­ dá»¥ng cÃ¢u src lÃºc dá»‹ch

![](/static/images/fundamentals-of-transformer/decoder_visualize.png)

á» vÃ­ dá»¥ nÃ y mÃ¬nh visualize decoder layer sá»‘ 2, táº¡i 4 heads 0,1,2,3. CÃ¡c báº¡n cÃ³ thá»ƒ quan sÃ¡t Ä‘Æ°á»£c khi encode tá»« **Ä‘Ã¬nh** mÃ´ hÃ¬nh sáº½ nhÃ¬n vÃ o cÃ¡c tá»« káº¿ cáº¡nh lÃ  **gia** vÃ  **tÃ´i**, (vÃ  cÃ²n nhiá»u kiá»ƒu pattern khÃ¡c ná»¯a nhÃ©). CÃ²n khi dá»± Ä‘oÃ¡n tá»« **tÃ´i** mÃ´ hÃ¬nh sáº½ nhÃ¬n vÃ o tá»« **my**.

# Tham kháº£o
[Positional encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)

[Bert Distilling](https://towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77)

[Transformer Visualize](http://jalammar.github.io/illustrated-transformer/)
